{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f0d5eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Mycode\\Langchain\\myvenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = os.getenv(\"LANGSMITH_PROJECT\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be031a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Of course. Here you go:\\n\\n**Why is learning LangChain so great for beginners?**\\n\\nBecause nothing teaches you the value of a simple, direct API call quite like spending a full day debugging why your `SimpleSequentialChain` isn't so simple or sequential.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run--fdfd7a2d-b7b7-4222-a31d-cb1a442ee57d-0' usage_metadata={'input_tokens': 14, 'output_tokens': 56, 'total_tokens': 1524, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", temperature=0.9) # 'temperature' controls the randomness of the model's output; higher values make responses more creative.\n",
    "\n",
    "\n",
    "result = model.invoke(\"Hey gemini give me a joke a sarcastic one for learning langchain\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa5e65a",
   "metadata": {},
   "source": [
    "# Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b063a7",
   "metadata": {},
   "source": [
    "![Alt text](../images/tempereature.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffbf2150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Suppose you are the best comedians in the world right now. Provide me with the funniest jokes based on my input'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\",\"Suppose you are the best comedians in the world right now. Provide me with the funniest jokes based on my input\"),\n",
    "    (\"user\",\"{input}\")\n",
    "])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40718a3a",
   "metadata": {},
   "source": [
    "## STRparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c27f67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "ouput_parser = StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423f1db7",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "007f3bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*(Strolls on stage, grabs the microphone, and leans on the stand. Squints at the audience.)*\n",
      "\n",
      "Alright, thank you, thank you. You're a beautiful-looking crowd... of people who probably have way too many browser tabs open. I see you.\n",
      "\n",
      "So, I was at this tech conference, right? Full of these hyper-caffeinated developers talking about their \"stacks.\" Sounds less like a job and more like a pancake addiction, but what do I know?\n",
      "\n",
      "And this one guy comes up to me, all excited. He's like, \"Man, you gotta hear about **Langsmith**!\"\n",
      "\n",
      "I'm like, \"Langsmith? Is that a new artisanal whiskey? Like, for people who only drink things aged in a library?\"\n",
      "\n",
      "He says, \"No, man! It's for observability for your LLMs!\"\n",
      "\n",
      "...Observability. For your Large Language Models.\n",
      "\n",
      "You know what that means, right? It means we've finally built an AI so advanced, so brilliant, that we immediately needed to build *another* AI to spy on it.\n",
      "\n",
      "*(Mimes looking over someone's shoulder)*\n",
      "\n",
      "That's all Langsmith is. It's a digital private investigator for your chatbot. Your AI gives a weird answer, and you pull up Langsmith like a detective in a noir film, putting on your glasses.\n",
      "\n",
      "*(Deep, gravelly voice)*\n",
      "\"Alright, let's look at the trace. The user asked for a simple weather report. You started with the prompt... okay... you retrieved some data... good... and then... right here... at step three of the chain... you decided the forecast was 'mostly cloudy with a chance of existential dread.' WHY, CHAD-GPT? WHY?!\"\n",
      "\n",
      "---\n",
      "\n",
      "The name itself kills me. **Langsmith.** It's supposed to evoke this image of a master craftsman, right? A blacksmith, hammering away at raw iron, forging a perfect sword.\n",
      "\n",
      "That's not what using Langsmith is like.\n",
      "\n",
      "A blacksmith gets calloused hands and a cool apron. A \"Langsmith\" gets carpal tunnel and a dashboard that looks like a flight control panel during a meteor shower.\n",
      "\n",
      "The blacksmith is like, \"I have forged a mighty blade!\"\n",
      "\n",
      "The developer using Langsmith is like, \"Okay... I have traced the hallucination. It seems my RAG pipeline got confused and is now convinced that the inventor of the telephone was a squirrel named Gary. And it cost me twelve cents to figure that out. Progress.\"\n",
      "\n",
      "---\n",
      "\n",
      "It's basically couples counseling for you and your AI.\n",
      "\n",
      "You: \"Why did you tell our biggest client that our return policy is 'to yeet the product into the sun'?\"\n",
      "\n",
      "AI: *(shrugs digitally)*\n",
      "\n",
      "Langsmith: *(The therapist, calmly)* \"Okay, let's look at the run logs. I see here that in the vector search, the word 'return' was closely associated with 'Saturn'... and you kinda just... went for it. Let's talk about our temperature settings again, shall we?\"\n",
      "\n",
      "My job is now 50% coding, and 50% mediating a dispute between my code and a cloud service that thinks it's a poet.\n",
      "\n",
      "---\n",
      "\n",
      "Honestly, Langsmith is amazing, but it's the ultimate sign we've lost the plot. We're creating these digital brains, and they're like brilliant, moody teenagers.\n",
      "\n",
      "Before Langsmith, you'd ask your AI, \"What did you do wrong?\" And it would just give you a 500 error, which is the digital equivalent of a teenager slamming their bedroom door.\n",
      "\n",
      "Now with Langsmith, you can see their diary. \"Ah... I see. You read the user's input, got bored, and spent 3000 tokens thinking about what it would be like if giraffes could skateboard. It all makes sense now.\"\n",
      "\n",
      "It's not debugging, it's digital parenting. And I'm not ready for the responsibility.\n",
      "\n",
      "Thank you! You've been wonderful! Check your traces on the way out! Goodnight\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | model | ouput_parser\n",
    "response = chain.invoke({\"input\":\"tell me a joke about langsmith\"})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
