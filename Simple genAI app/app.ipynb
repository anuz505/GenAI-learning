{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2733217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"radiohead\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9acbdc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "# docs = WebBaseLoader(\"https://react.dev/learn/describing-the-ui#your-ui-as-a-tree\")\n",
    "# finalDocs = docs.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5d5c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf\")\n",
    "finalDocs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f63b1669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "docs = splitter.split_documents(finalDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d10f9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba62ac41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 0, 'page_label': 'i'}, page_content='BinHero: An AI-Driven Smart W aste Disposal\\nMobile App with Object Detection Motion T racking\\nA report submitted in partial fulfilment of the requirements\\nfor the award of the degree of\\nBachelor\\nof\\nComputer Science (Hons)\\nby\\nANUJ BHANDARI\\n0358445\\nCOMPUTER SCIENCE\\nINSTITUTE OF INTERNA TIONAL MANAGEMENT SCIENCE (IIMS)\\nAFFILIA TED TO T A YLOR’S UNIVERSITY, MALA YSIA\\nKathmandu, Nepal\\nApril, 2025'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 1, 'page_label': 'i'}, page_content='CER TIFICA TE\\nThis is to certify that Anuj Bhandari, Bachelor (Computer Science), has worked on\\nthe project entitled ‘BinHero: An AI-Driven Smart W aste Disposal Mobile App\\nwith Object Detection Motion T racking ’ under my supervision and guidance. The\\ncontent of this report is original and has not been submitted elsewhere for the award of\\nany academic or professional degree.\\nApril, 2025 Sachin sir\\nIIMS College, Kathmandu Lecturer, Department of Computer Science\\nIIMS Col lege, Kathmandu\\ni'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 2, 'page_label': 'ii'}, page_content='ACADEMIC INTEGRITY\\nI hereby declare that this capstone project is my own work and, to the best of my\\nknowledge, it contains no materials previously published or written by any other person,\\nor substantial proportions of material which have been accepted for the award of any\\nother degree at IIMS College or any other educational institution, except where due\\nacknowledgement is made in the project.\\nApril, 2025\\nIIMS College\\nAnuj Bhandari\\nii'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 3, 'page_label': 'iii'}, page_content='ACKNOWLEDGEMENT\\nI would like to sincerely thank Mr. Sachin Shrestha Sir, who has guided me beyond mea-\\nsure, motivated me, and provided me with useful criticism during the progression of this\\nproject. I owe the faculty of IIMS College my utmost gratitude in terms of their technical\\nsupport and informed learning environment that enabled this project. Special thanks to\\nall my team members, who contributed their efforts and support to the project on a two\\ndimensional front in the backend development, the front end, UI/UX design, and the\\nquality assurance front of the project. Lastly , I would also like to show my gratitude\\nto my family and friends, the unconditional support and assistance of which have been\\ninvaluable to me throughout this process.\\nAnuj Bhandari\\niii'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 4, 'page_label': 'iv'}, page_content='ABSTRACT\\nBinHero is a multiplatform mobile application that was conceptualized to address\\nsome of the current challenges in waste management by embracing artificial intelligence\\nand gamification. The system combines YOLO11 to detect objects in real-time and Deep-\\nSOR T/SOR T to track objects, and therefore ensures that waste disposal can be accurately\\nclassified and verified. The architecture integrates hybrid AI inference, which allows for\\non-device and on-server computation, hence keeping the flexibility and responsiveness to\\na maximum degree. Using React Native, MongoDB, Node.js, and Flask, the application\\ntransforms waste disposal into an intelligent and engaging process, all while gathering\\ndata on garbage bin locations that can guide community planning. The project is an\\nillustration of the synergistic usage of AI, civic engagement, and behavioral science to\\ncreate a smart, scalable process of waste disposal.\\niv'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 5, 'page_label': 'v'}, page_content='Contents\\nCertificate i\\nAcademic Integrity ii\\nAcknowledgement iii\\nAbstract iv\\nT able of Contents v\\nList of Symbols and Abbreviations vii\\nList of Figures vii\\nList of T ables viii\\n1 Introduction 1\\n1.1 Executive Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\\n1.2 Project Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\\n1.3 Project Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\\n1.4 Project Scope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.5 Project Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n1.6 Milestones and Deliverables . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n2 Related W orks 7\\n2.0.1 AI Powered W aste Classification using Deep Learning . . . . . . . 7\\n2.0.2 Lightweight and Edge-Optimized Models . . . . . . . . . . . . . . 8\\n2.0.3 Motion T racking and Disposal V erification . . . . . . . . . . . . . 9'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 5, 'page_label': 'v'}, page_content='2.0.2 Lightweight and Edge-Optimized Models . . . . . . . . . . . . . . 8\\n2.0.3 Motion T racking and Disposal V erification . . . . . . . . . . . . . 9\\n2.0.4 Integrated Detection and T racking F rameworks . . . . . . . . . . 9\\n2.0.5 Comparative Insights . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n3 System Analysis 12\\n3.1 SWOT analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n3.2 T echnical Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\nv'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 6, 'page_label': 'vi'}, page_content='3.2.1 AI/ML Algorithms and Model Selection . . . . . . . . . . . . . . 13\\n3.2.2 Mathematical F ormulation of YOLO11 . . . . . . . . . . . . . . . 16\\n3.2.3 Model Output . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n3.2.4 Loss F unction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n3.2.5 Performance Metrics . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n3.2.6 Performance Considerations and T rade-offs . . . . . . . . . . . . . 18\\n3.3 Requirement specification . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n3.3.1 F unctional Requirement . . . . . . . . . . . . . . . . . . . . . . . 18\\n3.3.2 Non-F unctional Requirements . . . . . . . . . . . . . . . . . . . . 19\\n4 System Design 20\\n4.1 System Architecture Diagram . . . . . . . . . . . . . . . . . . . . . . . . 20\\n4.1.1 High-Level Overview . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n4.1.2 F rontend and User Interaction Layer . . . . . . . . . . . . . . . . 20'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 6, 'page_label': 'vi'}, page_content='4.1.1 High-Level Overview . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n4.1.2 F rontend and User Interaction Layer . . . . . . . . . . . . . . . . 20\\n4.1.3 Geolocation and Real-Time Communication . . . . . . . . . . . . 21\\n4.1.4 AI Inference Architecture . . . . . . . . . . . . . . . . . . . . . . 21\\n4.1.5 AI Pipeline W orkflow . . . . . . . . . . . . . . . . . . . . . . . . 22\\n4.1.6 Backend and Database . . . . . . . . . . . . . . . . . . . . . . . . 23\\n4.2 Use Case Diagram, UML Diagram, and Other Visual Representations . . 24\\n4.2.1 Use Case Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\n4.3 Class Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n4.4 Activity Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n4.5 Sequence Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n4.6 Pseudocode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n5 Conclusion 36\\nReferences 38'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 6, 'page_label': 'vi'}, page_content='4.5 Sequence Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n4.6 Pseudocode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n5 Conclusion 36\\nReferences 38\\nAppendices 39\\nA Gantt Chart and Current Progress 40\\nA.1 Gantt Chart . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\nA.2 Current Progress and Preliminary Results . . . . . . . . . . . . . . . . . 41\\nB Log Sheet 42\\nvi'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 7, 'page_label': 'vii'}, page_content='List of Symbols & Abbreviations\\nAI Artifical intel ligence\\nAPI Application Programming Interface\\nCNN Convolutional Neural Network\\nmAP Mean A verage Precision\\nmAP Mean Average Precision\\nML Machine learning\\nPvP Player vs Player\\nR-CNN Regions with Convolutional Neural Network\\nRe-ID Re-identification\\nTfLite T ensorFlow Lite\\nYOLO Y ou Only Look Once\\nvii'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 8, 'page_label': 'viii'}, page_content='List of Figures\\n1.1 Organization Chart . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n3.1 SWOT analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n3.2 YOLO11 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n4.1 System Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n4.2 Use Case Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n4.3 Class Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n4.4 User Authentication Activity Diagram . . . . . . . . . . . . . . . . . . . 28\\n4.5 PvP Challenges Activity Diagram . . . . . . . . . . . . . . . . . . . . . . 30\\n4.6 W aste Disposal V alidation Activity Diagram . . . . . . . . . . . . . . . . 31\\n4.7 Sequence Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\nA.1 Project Milestone Gantt Chart . . . . . . . . . . . . . . . . . . . . . . . 40'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 8, 'page_label': 'viii'}, page_content='4.7 Sequence Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\nA.1 Project Milestone Gantt Chart . . . . . . . . . . . . . . . . . . . . . . . 40\\nA.2 YOLO11 T raining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\nB.1 Meeting 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\\nB.2 Meeting 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\\nB.3 Meeting 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\nB.4 Meeting 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\nB.5 Meeting 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\nB.6 Meeting 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\\nB.7 Meeting 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\\nB.8 Meeting 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 8, 'page_label': 'viii'}, page_content='B.7 Meeting 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\\nB.8 Meeting 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\\nB.9 Meeting 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\nB.10 Meeting 10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\\nviii'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 9, 'page_label': 'ix'}, page_content='List of T ables\\n1.1 Roles and Responsibilities . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n1.2 Milestones and Deliverables . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.1 Summary of Related W orks . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n3.1 Algorithm and Model Selection Summary and Justification . . . . . . . . 16\\n3.2 Comparison of AI components in BinHero . . . . . . . . . . . . . . . . . 18\\nA.1 Current Performance of Detection Model . . . . . . . . . . . . . . . . . . 41\\nix'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 10, 'page_label': '1'}, page_content='Chapter 1\\nIntroduction\\n1.1 Executive Summary\\nRapid urbanization and city growth have caused a significant increase in worldwide waste.\\nBy 2050, this production is anticipated to increase by 70%, per Kaza et al. (2018). W aste\\nmanagement techniques in the past had certain issues that made it challenging for both\\npeople and communities to participate and recycle properly . In order to confront this\\nmultifaceted challenge, the application presents a novel approach and tackles the issue\\nusing crowdsourcing, gamification, and artificial intelligence as features within a single\\nmobile application.\\nBinHero is a mobile application that is powered by AI technologies like object detection\\nand motion tracking. These AI technologies help validate trash disposal actions in real-\\ntime, while also promoting accurate sorting behavior with fast feedback. Point systems,\\nleaderboards, PvP challenges, and team-based tasks, such as gamified elements, further'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 10, 'page_label': '1'}, page_content='time, while also promoting accurate sorting behavior with fast feedback. Point systems,\\nleaderboards, PvP challenges, and team-based tasks, such as gamified elements, further\\nenhance user engagement and develop long-term habits. The application aims to turn\\na garbage disposal into a community-driven activity by incorporating those AI features\\nand enabling users to map and verify bin sites through crowdsourcing. This boosts\\naccessibility and aids municipal planning efforts. The app was built using React Native,\\nwith backend support from Node.js and MongoDB. Likewise, YOLO11 and DeepSOR T\\nalgorithms were applied to detect objects and track their motion.\\nA thorough risk assessment and development roadmap guarantee that challenges\\nfrom AI accuracy and hardware compatibility to user retention are addressed proactively\\nthrough continual model refinement, strategic outreach, and inclusive design. Likewise,'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 10, 'page_label': '1'}, page_content='from AI accuracy and hardware compatibility to user retention are addressed proactively\\nthrough continual model refinement, strategic outreach, and inclusive design. Likewise,\\nfuture improvements will be incorporated, like augmented reality integration, federated\\nlearning, multilingual support, and collaboration with government and environmental\\ngroups.\\nIn conclusion, the project illustrates some of the ways technology and behavioral\\nscience may work together to provide environmental benefits, encourage community in-\\nvolvement, and build a culture of sustainability .\\n1'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 11, 'page_label': '2'}, page_content='Anuj Bhandari: Capstone Project I\\n1.2 Project Objectives\\nThe mobile application aims to revolutionize waste management methods by powering\\nthem with AI and gamification. Some of the primary objectives are as follows:\\n• AI for Real-Time W aste Classification and V alidation: Make use of AI to track both\\nobjects and their movements to confirm proper waste disposal and help address\\nmistakes in sorting and waste added to landfills.\\n• Promote Community Participation in Sustainability Efforts: Collaboration with\\nenvironmentally conscious communities by offering team-based challenges, missions,\\nand PvP (Player vs Player) arenas.\\n• Scalable and Engaging Mobile Platform: The Project aims to provide a robust\\nbackend that can handle a high load of traﬀic and will perform in harsh conditions.\\n• Data Driven: Locations of dustbins from crowdsourcing can be beneficial for the\\nlocal government to plan and increase the eﬀiciency of garbage collection.\\n1.3 Project Description'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 11, 'page_label': '2'}, page_content='• Data Driven: Locations of dustbins from crowdsourcing can be beneficial for the\\nlocal government to plan and increase the eﬀiciency of garbage collection.\\n1.3 Project Description\\nThe project utilizes AI and user-driven engagement and encourages long-lasting envi-\\nronmental change, emerging as an effective solution to the ongoing global problem of\\ninappropriate garbage disposal. Key Components and F eatures:\\n• Gamification Mechanics:\\n– Garbage Disposal\\n– Daily and weekly missions.\\n– PvP contests.\\n– T eam challenges.\\n– Leaderboards.\\n– Badges and achievements.\\n• AI-powered Object detection and Motion tracking:\\n– Custom-trained YOLO11 model to classify wastes as recyclable, biodegrad-\\nable, or general.\\n• Crowdsourced Bin Mapping\\n– Users pin the location of trash bins to a live map\\n– Help the local government collect garbage eﬀiciently and promote public par-\\nticipation.\\n2'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 12, 'page_label': '3'}, page_content='Anuj Bhandari: Capstone Project I\\n• Backend Infrastructure\\n– A minimal and flexible Node.js framework for server-side operations.\\n– MongoDB is a NoSQL database to manage user profiles, bin data, and other\\ninteractions.\\n– AI models built using the Pytorch framework with cloud-based inference.\\n• Platform and Accessibility\\n– Built with React Native with usability and performance in mind, and opti-\\nmized for contemporary devices.\\n1.4 Project Scope\\nThe functional boundaries, features that the team intends to create and execute, are\\ndescribed in the application’s project scope. In order to guarantee clarity of purpose and\\ncompatibility with technical requirements. Here, both projects’ inclusions and exclusions\\nare described.\\nInclusion:\\n1. Mobile app development: Build a full-fledged mobile application from React Native\\nwith robust backend support that can provide users with seamless accessibility and\\nfunctionality .'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 12, 'page_label': '3'}, page_content='Inclusion:\\n1. Mobile app development: Build a full-fledged mobile application from React Native\\nwith robust backend support that can provide users with seamless accessibility and\\nfunctionality .\\n2. Gamification Rewards Systems: T o engage users better, tools like structured points,\\nrankings, battles between players, and challenges in teams are created and placed\\nin the applications.\\n3. Crowd-source: It helps residents contribute to their community and plan the use\\nof municipal resources by letting them update and check bin locations on the app.\\nExclusion\\n1. Integration with Government W aste Collection Systems: At present, there has been\\nno oﬀicial integration with the municipal garbage collection or the government,\\ndespite the fact that the system might support data that is beneficial to municipal\\norganizations.\\n2. Hardware Device: The project also does not entail the creation of physical hardware,\\nsuch as sensor-based disposal devices.'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 12, 'page_label': '3'}, page_content='organizations.\\n2. Hardware Device: The project also does not entail the creation of physical hardware,\\nsuch as sensor-based disposal devices.\\n3. Offline F unctionality: F eatures such as real-time data sync, multiplayer interactions\\nthese all require internet connectivity , so the app does not have any offline features.\\n3'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 13, 'page_label': '4'}, page_content='Anuj Bhandari: Capstone Project I\\n1.5 Project Organization\\nThe project consists of an organizational structure that guarantees effective teamwork\\nand accountability , which is essential for a successful execution of the project. The team\\ncomprises five members who contribute to their own specific domains and their areas of\\nexpertise.\\nGroup Structure : Under the direction of a group leader, every member leads or con-\\ntributes to an area of their expertise and works closely with others in a flat but functional\\norganization.\\nAAYUSH KARKI\\nGroup Leader/Backend\\nAAYUSH BASNET\\nUI/UX\\nPRABIN JOSHI\\nFrontend\\nANUJ BHANDARI\\nAI/ML\\nYANGMA LAMA\\nQA/Frontend\\nFigure 1.1: Organization Chart\\nThe project organization involves the following roles:\\nGroup Leader : The Group Leader directs the workflow management activities and\\nworks to build teamwork among the members. The group leader assigns responsibilities\\nto team members while maintaining constant communication with the supervisor.'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 13, 'page_label': '4'}, page_content='works to build teamwork among the members. The group leader assigns responsibilities\\nto team members while maintaining constant communication with the supervisor.\\nGroup Members : Each member of the team brings various abilities to the group and\\nis dedicated to a certain component of the project. In order to accomplish job eﬀiciency\\nand system completion, users work together as a team.\\n1.6 Milestones and Deliverables\\nOur application’s development life cycle has been divided into several phases. It outlines\\nall those critical stages and milestones that are required to successfully complete the\\napplication’s development. The sole purpose of these plans is to track progress, allocate\\n4'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 14, 'page_label': '5'}, page_content='Anuj Bhandari: Capstone Project I\\nT able 1.1: Roles and Responsibilities\\nName Roles Responsibility\\nAnuj Bhandari AI/ML Development of Object detection model and collaboration\\nwith Backend to integrate models.\\nAayush Karki Backend Development W ork on the server-side logic and database connection.\\nAayush Basnet UI/UX Design an aesthetically pleasing and user-friendly interfaces\\nPrabin Joshi F rontend DevelopmentConverts UI/UX designs and implements responsive, inter-\\nactive interfaces of mobile applications.\\nY angma LamaQA/F rontend DevelopmentCarries out testing and guarantees the functionality of the\\nsoftware while also assisting frontend development.\\nresources, and on on-time delivery . The significant milestones and anticipated timescale\\nare shown in T able 2.1.\\nThe project timings and key achievement objectives in all tasks are represented in the\\nFigure. The Gantt chart helps to monitor the process in several stages, which include'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 14, 'page_label': '5'}, page_content='are shown in T able 2.1.\\nThe project timings and key achievement objectives in all tasks are represented in the\\nFigure. The Gantt chart helps to monitor the process in several stages, which include\\nrequirement analysis, design development, testing, and deployment. It ranges from March\\nto December. T eam members are highlighted with color bars to represent the expected\\ntime for their task; it encourages responsibility and cooperation.\\n5'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 15, 'page_label': '6'}, page_content='Anuj Bhandari: Capstone Project I\\nT able 1.2: Milestones and Deliverables\\nNo Milestone Expected time (Days )\\n1 T opic Research 10\\n2 Gathering and Analyzing Requirements 10\\n3 Project Proposal 10\\n4 UI/UX Design 45\\n5 Backend Development 55\\n6 AI model Development 35\\n7 F rontend Development 55\\n8 Gamification Implementation 25\\n9 CroudSource Bin Location 12\\n10 T esting 15\\n11 Documentation and Deployment 20\\n6'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 16, 'page_label': '7'}, page_content='Chapter 2\\nRelated W orks\\nOne of the most pressing environmental issues currently is the worldwide waste manage-\\nment catastrophe. According to Kaza et al. (2018), approximately 2.01 billion tons of\\nmunicipal solid waste is produced annually . Despite increasing environmental awareness,\\nthe public has shown inadequate responses to conventional waste management education\\nand infrastructure improvements.\\n2.0.1 AI Powered W aste Classification using Deep Learning\\nArtificial intelligence has been widely used in waste management in recent years due\\nto the growing demand for accurate and eﬀicient techniques to handle urban garbage.\\nObject detection and classification are essential components of contemporary waste man-\\nagement systems. Arishi (2025) applied YOLOv8 on her paper to accurately classify\\nhousehold waste. With a mean average precision (mAP) of 89.5%, the YOLOv8-CBAM\\nmodel outperformed the baseline model by a significant 4.2%. The paper implemented'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 16, 'page_label': '7'}, page_content='household waste. With a mean average precision (mAP) of 89.5%, the YOLOv8-CBAM\\nmodel outperformed the baseline model by a significant 4.2%. The paper implemented\\ncomparative analysis of other state of the art models such as YOLOv7, EﬀicientDet, F ast\\nR-CNN and MaskR-CNN .In terms of performance and accuracy YOLOv8 was able to\\noutperform all of them. The paper focused on the lightweight networks practical and\\nreal-time applications on consumer devices.\\nKuang et al. (2024) study achieved a success rate accuracy of 89.5% as well. The\\npaper addressed the problem of ineffective sorting and classification of waste still remains\\nan ineﬀicient waste management dimension in terms of time and cost consumption in the\\nentire process. The research also identified the potentials as well as the limitations that\\nfaced YOLO in practical contexts, where it was sensitive to complicated backgrounds\\nand overtopped objects. Nevertheless, the model demonstrated a high accuracy rate of'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 16, 'page_label': '7'}, page_content='faced YOLO in practical contexts, where it was sensitive to complicated backgrounds\\nand overtopped objects. Nevertheless, the model demonstrated a high accuracy rate of\\nclassification in controlled conditions despite such challenges.\\nNafiz et al. (2023) introduced Convowaste, a deep learning-driven system of trash\\nsegregation. It applied transfer learning with Inception-ResNet V2 to just label garbage\\ninto six categories with 98 percent of accuracy . The accuracy of the model on the avail-\\nable limited labeled data was astounding, which reflects on the success of pre-trained\\nconvolutional architectures in ensuring accurate classification in practical settings. Their\\n7'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 17, 'page_label': '8'}, page_content='Anuj Bhandari: Capstone Project I\\napproach, also, exhibited fast inference which is crucial to low-data real-time systems.\\nSayem et al. (2025) suggested strong waste classification and detection models that\\nallow eﬀicient sorting and recycling of waste. The paper presented a with a dataset called\\nW aRP which consists a collection of realistic conditions of waste types (28 in total) with\\nvarious low light conditions and occlusion. The dataset was trained on GELAN-E de-\\ntector which obtained 63% mAP@50 and on their dual-stream Dense ViT model, which\\nobtained 83.1% accuracy , outperforming the results of the prior datasets like T rashNet.\\nThis showed the influence of environment specific data on the model generalization.\\nAlharbi et al. (2025) addressed authors consider the burning issue of garbage in streets\\nand the lack of eﬀicient waste disposal in the world and proposed a solution called SA WN\\n(Surveillance and W aste Notification) system. An innovatice approach in which real-time'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 17, 'page_label': '8'}, page_content='and the lack of eﬀicient waste disposal in the world and proposed a solution called SA WN\\n(Surveillance and W aste Notification) system. An innovatice approach in which real-time\\nvideo classification and object detection are combined to detect and address cases of lit-\\ntering. Authors emphasized with dataset constraints in relation to garbage detection and\\npublic surveillance. Despite understanding that there is a need to personalize datasets\\nwith contextual behavioral labels, i.e., the type of littering (throwing, dropping, discard-\\ning), in combination with object instances (trash bags, bottles), their model was designed\\nto use multiple detection models (YOLOv8, MoViNet, and Haar Cascades). The SA WN\\nsystem illustrated high accuracy , it achieved 99.5% accuracy score with MoViNet and\\n99.5% accuracy score with YOLOv8 to detect license plate.\\nBesides limitations of datasets, uncontrolled conditions are an essential issue regard-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 17, 'page_label': '8'}, page_content='99.5% accuracy score with YOLOv8 to detect license plate.\\nBesides limitations of datasets, uncontrolled conditions are an essential issue regard-\\ning robustness. Abo-Zahhad and Abo-Zahhad (2025) remedied this by cross-comparative\\nanalysis of YOLOv5 and YOLOv8 to identify the overflow of bins through the presence\\nand absence of light, occlusions, and clutters. 95% and 96% accuracy were attained in\\ntheir models respectively . F easibility of running those models on low-resource and mobile\\nhardware was also confirmed by the study , which means that the models can be used\\nin the field in real-time fashion. The results encourage the foundational application of\\nadaptive models that are well generalizable to the variability of the environmental real\\nworld.\\n2.0.2 Lightweight and Edge-Optimized Models\\nThe core of project’s technical orientation is dealing with real-time inference on de-\\nvices with eﬀiciency .Nedjar et al. (2024) utilized lightweight models like MobileNet and'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 17, 'page_label': '8'}, page_content='The core of project’s technical orientation is dealing with real-time inference on de-\\nvices with eﬀiciency .Nedjar et al. (2024) utilized lightweight models like MobileNet and\\nNASNet-Mobile, on systems like real-time solid waste sorter. The model was able to\\nachieve nearly 100% classification accuracy and operated on Raspberry Pi with T ensor-\\nFlow lite for eﬀicient edge deployment. Likewise, Prasath (2025) verified that YOLO are\\nappropriate for embedded systems by testing YOLOv5 on Jetson Nano and Raspberry\\nPi. It balanced accuracy and processing performance. Additionally , IoT-based robotic\\narm was integrated into the system to physically sort into respective bins. Their system\\n8'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 18, 'page_label': '9'}, page_content='Anuj Bhandari: Capstone Project I\\nwas scalable, cost effective and deployed as a framework of AI-based trash management\\nspecifically for smart cities and industrial automation in India\\nWhite et al. (2020) implemented deep convolutional neural network tailored to edge\\ndevices. W asteNet deploys its model on jetson Nano since it is low-resource and has\\nsuccessfully achieved 97% accuracy . The research paper aimed to enhance waste sorting\\neﬀiciency by using an automated, real-time waste sorting system, which would help solve\\nthe problem of ever-expanding loads of garbage and increasing contamination of recy-\\ncled commodities. The paper also explored the AI-based approach to waste management\\nthrough the implementation of object detection solution that can be used to categorize\\nthe trash. In this regard, pretrained on ImageNet DenseNet model was implemented.\\nThe model was evaluated on T rashNet dataset and achieved an accuracy score of 97%.\\n2.0.3 Motion T racking and Disposal V erification'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 18, 'page_label': '9'}, page_content='The model was evaluated on T rashNet dataset and achieved an accuracy score of 97%.\\n2.0.3 Motion T racking and Disposal V erification\\nKim and Cho (2022) proposed AIDM-strat which aimed at reducing the illegal garbage\\ndumping, which is of significant concern in the Republic of Korea and is made even worse\\nby the lack of municipal waste management system based on the volume. AIDM-strat\\nimplemented a combination of YOLOv4 and OpenPose to track wrist-to-object distances\\nduring disposal. This approach allows for accurate behavioral analysis in situations that\\ninvolve illegal dumping. In order to confirm dumping, the system detects the trash and\\nthe distance between the hand and the trash, making it possible to detect behaviors\\nlike deliberate dumping. It was trained on GPU and scored a high mAP of 99.38% for\\ntrash bag classification. AIDM-Strat offered a more intelligent and precise solution to\\nthe problem of urban garbage, since it was able to evaluate the location of an object, in'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 18, 'page_label': '9'}, page_content='trash bag classification. AIDM-Strat offered a more intelligent and precise solution to\\nthe problem of urban garbage, since it was able to evaluate the location of an object, in\\naddition to identifying it, to allow a groundbreaking mechanism to recognize behavior\\ndepending on the context.\\n2.0.4 Integrated Detection and T racking F rameworks\\nHuang et al. (2023) proposed a study on a real-time automotive sensing system and\\ndesigned the solution to monitor urban garbage disposal data.The system consists the in-\\ntegration of object detection (YOLOv5) with tracking algorithms like DeepSOR T could\\nbe used to monitor and analyze waste related activities across time. with the use of\\nMQTT protocols for real-time data handling the system, which was installed in garbage\\ntrucks achieved 16.52 FPS and low latency communication (76ms). Similarly , Pathak\\net al. (2024) has used a multi component AI system that included T esseract OCR for li-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 18, 'page_label': '9'}, page_content='trucks achieved 16.52 FPS and low latency communication (76ms). Similarly , Pathak\\net al. (2024) has used a multi component AI system that included T esseract OCR for li-\\ncense plate recognition, and YOLOv5 for object detection. It could identify the amounts\\nof unlawful dumping with the detection rate of 97 percent. The system is an illustration\\nof a comprehensive surveillance system that can identify complex environmental. The\\narticle has also discussed dataset construction and preparation in detail in order to en-\\n9'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 19, 'page_label': '10'}, page_content='Anuj Bhandari: Capstone Project I\\nhance replicability and resilience in the dataset.\\n2.0.5 Comparative Insights\\nA comparison of previous AI/ML models for smart trash management uncovers few im-\\nportant patterns and trade-offs that would influence the BinHero system’s architecture.\\nAmong the reviewed object detection algorithm YOLO models were outstanding in real-\\ntime classification of garbage. Arishi (2025) and Abo-Zahhad and Abo-Zahhad (2025)\\nrecord its greater precision and its applicability to edge deployment, thus validating the\\nchoice of mobile application to run YOLO11 on its servers and YOLO11n for mobile\\ninference.\\nDeepSOR T provides large performance advantages in terms of re-identification but re-\\nquires a graphics processing unit (GPU) as opposed to SOR T, which is fast and favors\\ntracking on mobile with less opportunity to handle occlusion. In practice, the selective\\nusage of both systems is used in the project where they are selected depending on the'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 19, 'page_label': '10'}, page_content='tracking on mobile with less opportunity to handle occlusion. In practice, the selective\\nusage of both systems is used in the project where they are selected depending on the\\nrequirements of the task in hand.PyT orch allows significant flexibility in model building\\nbut adds more complexities to frameworks deployed in resource-constrained platforms.\\nIn conclusion, dual-inference and hybrid tracking strategy integrates the most impor-\\ntant aspects of previous research, while balancing accuracy , performance, and platform\\ncompatibility .framework\\n10'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 20, 'page_label': '11'}, page_content='Anuj Bhandari: Capstone Project I\\nT able 2.1: Summary of Related W orks\\nStudy Problem Addressed AI Approach Deployment Approach Key Result Relevance to BinHero\\nArishi(2025) accurate classification of\\nhousehold garbage\\nYOLOv8-CBAM Consumer devices 89.5% mAP Ensures the application of YOLO\\non-device real-time classification\\nKuang et al.(2024) YOLO performance in practi-\\ncal settings\\nYOLOv8 Controlled vs real environ-\\nments\\nHigh accuracy in clean settingsEndorses the real-time applicability\\nof YOLO\\nWhite et al.(2020) On-device waste classificationW asteNet (CNN) Jetson Nano 97% accuracy Evidence of low- resource edge de-\\nployment\\nNafiz et al.(2023) Multi-category trash classifi-\\ncation\\nInception-ResNet V2T ransfer learning on limited\\ndata\\n98% accuracy Confirms pre-trained CNNs on\\nsmall datasets\\nNedjar et al.(2024) Light models for sorting on\\nembedded systems\\nMobileNet, NASNet-\\nMobile\\nRaspberry Pi with TFLite Nearly 100% accuracyV alidates lightweight models for em-\\nbedded inference'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 20, 'page_label': '11'}, page_content='Nedjar et al.(2024) Light models for sorting on\\nembedded systems\\nMobileNet, NASNet-\\nMobile\\nRaspberry Pi with TFLite Nearly 100% accuracyV alidates lightweight models for em-\\nbedded inference\\nPrasath(2025) Real-time YOLO use in em-\\nbedded systems\\nYOLOv5 Jetson Nano Balanced accuracy-performanceAﬀirms the appropriateness of\\nYOLO to smartphones\\nKim and Cho(2022) Behavior analysis in illegal\\ndumping\\nYOLOv4 + OpenPoseGPU system 99.38% mAP for trash bagsInspires motion + classification\\nstrategy in BinHero\\nHuang et al.(2023) W aste monitoring over timeYOLOv5 + DeepSOR TGarbage trucks + MQTT16.52 FPS, 76ms latencyDemonstrates object tracking\\npipeline integration\\nPathak et al.(2024) Illegal dumping identificationYOLOv5 + OCR Surveillance system 97% detection rate Illustrates multi-model synergy in\\nwaste tracking\\nSayem et al.(2025) Dataset realism and robust-\\nness\\nGELAN-E, Dense ViTW aRP dataset 83.1% accuracy Supports contextual dataset impor-\\ntance'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 20, 'page_label': '11'}, page_content='waste tracking\\nSayem et al.(2025) Dataset realism and robust-\\nness\\nGELAN-E, Dense ViTW aRP dataset 83.1% accuracy Supports contextual dataset impor-\\ntance\\nAlharbi et al.(2025) Behavioral trash detectionYOLOv8, MoViNet,\\nHaar\\nPublic surveillance Multi-model integrationEmphasizes need for behavioral\\ndatasets\\nAbo-Zahhad and Abo-Zahhad(2025) Overflow detection in clut-\\ntered conditions\\nYOLOv5 vs YOLOv8Low-resource/mobile hard-\\nware\\n95–96% accuracy V alidates YOLO under occlusion\\nand low light\\n11'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 21, 'page_label': '12'}, page_content='Chapter 3\\nSystem Analysis\\n3.1 SWOT analysis\\nThis section aims to provide a SWOT analysis of key artificial intelligence (AI) and ma-\\nchine learning components that underlie the application system. Figure 3.1 consists of all\\nthe key strengths, weaknesses, opportunities, and threats of the core AI/ML components\\nused in the project.\\nLightweight; real-time on \\nmobile\\nFast tracking; mobile-\\nfriendly\\nHigh accuracy with re-ID \\nsupport\\nFull training-to-deployment \\nsupport\\nLow-latency mobile inference\\nSupports both server and \\nmobile processing\\nFast, accurate detection Needs GPU; not for mobile\\nLower accuracy; limited for \\ncomplex scenes\\nOf fline use; supports TFLite \\nand mobile accelerators\\nEnables of fline tracking\\nUseful in multi-user detection \\nscenarios\\nExports to ONNX/TFLite; \\nwidely adopted\\nGood for of fline and older \\nmobile devices\\nImproves flexibility and \\nreliability\\nAPI/version compatibility \\nissues\\nPoor performance on \\noutdated hardware\\nOutput dif ferences may \\nreduce user trust'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 21, 'page_label': '12'}, page_content='Good for of fline and older \\nmobile devices\\nImproves flexibility and \\nreliability\\nAPI/version compatibility \\nissues\\nPoor performance on \\noutdated hardware\\nOutput dif ferences may \\nreduce user trust\\nPerformance drops under \\nhigh load\\nW eak in crowded or fast-\\nmotion scenarios\\nDependent on device \\nhardware; model drift risk\\nPoor with occlusion and \\nre-ID\\nHeavy GPU usage; not \\nmobile-compatible\\nHard to combine with \\nmobile tech stacks\\nNeeds tuning; limited custom \\nlayer support\\nAdds system complexity\\nSuitable for cloud-based \\nreal-time processing\\nImpacted by server issues; \\nneeds retraining\\nStr engthsT echnology W eaknesses Opp ort u n i t i es Thr eats\\nY OLOv 1 1  \\n( Server )\\nY OLOv 1 1 n \\n(M obile )\\nSO R T\\nDeepSO R T  \\n( Server )\\nPyT orch\\nTFLite\\nDual Inference\\nFigure 3.1: SWOT analysis\\n12'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 22, 'page_label': '13'}, page_content='Anuj Bhandari: Capstone Project I\\n3.2 T echnical Analysis\\nThe system combines the qualities of both artificial intelligence (AI) and machine learning\\n(ML) by introducing intelligent waste classification and disposal verification on the basis\\nof the motion to the mobile application. The architecture was intentionally designed to\\nachieve the best trade-offs can be made between the high detection accuracy , the low-\\nlatency inference, and scalability across both the mobile and the server environments.\\n3.2.1 AI/ML Algorithms and Model Selection\\nThe project is based on two fundamental machine-learning modules: object detection and\\nmotion tracking. T o accomplish these objectives, models must be selected that provide\\nthe optimal trade offs between accuracy , real-time inference, model size, and resource\\neﬀiciency . Accordingly , YOLO11 and YOLO11n were selected for object detection, while\\nDeepSOR T and SOR T to be used as part of the tracking framework, these decision were'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 22, 'page_label': '13'}, page_content='eﬀiciency . Accordingly , YOLO11 and YOLO11n were selected for object detection, while\\nDeepSOR T and SOR T to be used as part of the tracking framework, these decision were\\nboth technical and strategic in terms of operational goals.\\nObject Detection\\nThe mobile app implements two state-of-the-art object detection models, YOLO11 and\\nYOLO11n. Since the project uses a hybrid inference approach, YOLO11 is integrated on\\nthe server for its accuracy , while YOLO11n is deployed on mobile for its lightweight ar-\\nchitecture. These models were able to perform single-stage detection, which allowed real-\\ntime classification of two primary categories of waste: biodegradable and non-biodegradable.\\nInternal W orking Principles of YOLO11\\n• Grid Division: YOLO11 divides the input picture into a grid of S S 2. In this grid,\\nevery grid cell has a specific task of predicting bounding boxes and the respective\\nconfidence scores when the center of an object has fallen within a grid cell.'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 22, 'page_label': '13'}, page_content='every grid cell has a specific task of predicting bounding boxes and the respective\\nconfidence scores when the center of an object has fallen within a grid cell.\\n• Bounding Box Prediction: Each grid cell predicts bounding boxes, which consist of\\ncoordinates (x,y), width(w) ,and height(h). It also consists of a confidence score\\nthat indicates the object occurrence and the bounding box accuracy .\\n• Class Prediction: The model predicts the probabilities of classes for each bounding\\nbox.\\nInternal Architecture of YOLO11\\nAs outlined by Khanam and Hussain (2024), YOLO11 is a state-of-the-art object detector\\nthat is based on the YOLO ”Y ou Only Look Once” architecture. It is known for its single\\nforward pass detection, which makes it significantly faster compared to other detectors\\nlike R-CNN. The key components of YOLO11 are as follows:\\n1. CSP-based Backbone : The YOLO11 uses feature extraction based on Cross\\nStage Partial networks. C3K2 block was the main innovation since it was more\\n13'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 23, 'page_label': '14'}, page_content='Anuj Bhandari: Capstone Project I\\nR.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\nmodel’s applicability in various domains. YOLOv11’s design focuses on balancing power and practicality, aiming to\\naddress specific challenges across various industries with increased accuracy and efficiency.\\nThis latest model demonstrates the ongoing evolution of real-time object detection technology, pushing the boundaries\\nof what’s possible in CV applications. Its versatility and performance improvements position YOLOv11 as a significant\\nadvancement in the field, potentially opening new avenues for real-world implementation across diverse sectors.\\n4 Architectural footprint of Yolov11\\nThe YOLO framework revolutionized object detection by introducing a unified neural network architecture that\\nsimultaneously handles both bounding box regression and object classification tasks [17]. This integrated approach'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 23, 'page_label': '14'}, page_content='simultaneously handles both bounding box regression and object classification tasks [17]. This integrated approach\\nmarked a significant departure from traditional two-stage detection methods, offering end-to-end training capabilities\\nthrough its fully differentiable design.\\nAt its core, the YOLO architecture consists of three fundamental components. First, the backbone serves as the primary\\nfeature extractor, utilizing convolutional neural networks to transform raw image data into multi-scale feature maps.\\nSecond, the neck component acts as an intermediate processing stage, employing specialized layers to aggregate\\nand enhance feature representations across different scales. Third, the head component functions as the prediction\\nmechanism, generating the final outputs for object localization and classification based on the refined feature maps.\\nBuilding on this established architecture, YOLO11 extends and enhances the foundation laid by YOLOv8, introducing'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 23, 'page_label': '14'}, page_content='Building on this established architecture, YOLO11 extends and enhances the foundation laid by YOLOv8, introducing\\narchitectural innovations and parameter optimizations to achieve superior detection performance as illustrated in Figure\\n1. The following sections detail the key architectural modifications implemented in YOLO11:\\nFigure 1: Key architectural modules in YOLO11\\n4.1 Backbone\\nThe backbone is a crucial component of the YOLO architecture, responsible for extracting features from the input\\nimage at multiple scales. This process involves stacking convolutional layers and specialized blocks to generate feature\\nmaps at various resolutions.\\n4.1.1 Convolutional Layers\\nYOLOv11 maintains a structure similar to its predecessors, utilizing initial convolutional layers to downsample the\\nimage. These layers form the foundation of the feature extraction process, gradually reducing spatial dimensions while'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 23, 'page_label': '14'}, page_content='image. These layers form the foundation of the feature extraction process, gradually reducing spatial dimensions while\\nincreasing the number of channels. A significant improvement in YOLO11 is the introduction of the C3k2 block,\\nwhich replaces the C2f block used in previous versions [ 18]. The C3k2 block is a more computationally efficient\\nimplementation of the Cross Stage Partial (CSP) Bottleneck. It employs two smaller convolutions instead of one large\\nconvolution, as seen in YOLOv8 [13]. The \"k2\" in C3k2 indicates a smaller kernel size, which contributes to faster\\nprocessing while maintaining performance.\\n4.1.2 SPPF and C2PSA\\nYOLO11 retains the Spatial Pyramid Pooling - Fast (SPPF) block from previous versions but introduces a new Cross\\nStage Partial with Spatial Attention (C2PSA) block after it [18]. The C2PSA block is a notable addition that enhances\\n3\\nFigure 3.2: YOLO11 Architecture\\ncomputationally eﬀicient varient of the CSP bottleneck. This improved precsion'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 23, 'page_label': '14'}, page_content='3\\nFigure 3.2: YOLO11 Architecture\\ncomputationally eﬀicient varient of the CSP bottleneck. This improved precsion\\nand speed while also maintaining parameter eﬀiciency .\\n2. SPPF (Spatial Pyramid Pooling-F ast: SPPF captures contextual information\\nat multiple receptive fields hence allowing the network to process objects form\\ndifferent scales.\\n3. C2PSA (Cross Stage Partial with Spatial Attention) : C2PSA block en-\\nhances the model by introducing spatial attention mechanism. Utilization of spa-\\ntial information helps a model allocate computation power towards the significant\\nparts of an image, a fact which intensifies the detection performance, particularly\\nwith small or close targets.\\n4. Neck F eature Aggregation: YOLO11 is an evolutionary development of the ini-\\ntial YOLO architecture. In addition to utilizing optimized neck architecture with\\nC3k2 block to serve as an aggregation and fine-tuning of the multi-scale feature rep-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 23, 'page_label': '14'}, page_content='tial YOLO architecture. In addition to utilizing optimized neck architecture with\\nC3k2 block to serve as an aggregation and fine-tuning of the multi-scale feature rep-\\nresentation, it also integrates self-attention mechanism further to become a fusion\\nand a feature distinguish modal.\\n5. Detection Head : Multiple C3k2 blocks is utilized to process the aggregated fea-\\nture and output bounding box coordinates, objectness scores and class probabilites.\\n6. Loss F unction: YOLO11 utilizes CloU losses for precise bounding box regression\\nand focal loss.\\nYOLO11n for Mobile inference\\nYOLO11n is a quantized and compressed version of the YOLO11. It consists of:\\n• Smaller convolutional kernels and less number of layers.\\n• Diminished width and depth multipliers.\\n• T ensorFlow compatibility which allows for acceleration.\\n14'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 24, 'page_label': '15'}, page_content='Anuj Bhandari: Capstone Project I\\nJustification for BinHero\\nThe selection of these algorithms and models are based on:\\n• The project is dependent on real-time classification as a core business in order to\\nprovide real-time feedback to its users.\\n• YOLO11 provides high quality , reliable performance in closed, server-based condi-\\ntions where GPU adequate resources are available.\\n• YOLO11n makes mobile classification possible even for mid-range devices. While\\nalso providing offline support and a lower server burden.\\n• The application’s requirement to handle dynamic trash disposal actions in a mat-\\nter of seconds matches with its anchor-based infrastructure and the fast inference\\ncapabilities.\\nMotion T racking\\nWhile waste classification is an important factor for validating disposal behavior, it is\\nstill insuﬀicient on its own. The application empolys motion tracking with two tracking\\nalgorithms, DeepSOR T and SOR T, that links object IDs across frames, to make sure that'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 24, 'page_label': '15'}, page_content='still insuﬀicient on its own. The application empolys motion tracking with two tracking\\nalgorithms, DeepSOR T and SOR T, that links object IDs across frames, to make sure that\\nthe waste item is physically disposed of in the bin.\\nInternal mechanism of SOR T\\nSOR T which stands for Simple Online and Real-time T racking is a light weight tracking\\nalgorithm which utilizes classic computer vision methods.\\n• Input: It takes bounding boxes coordinates from YOLO models as input.\\n• Kalman Filter: Based on the object’s previous state, it predicts its future position.\\n• Hungarian Algorithm: Utilizes intersection over union (IoU) to match detection to\\npredicted tracks.\\n• T rack Management: Addition of new tracks and modify old ones and eliminate lost\\nones.\\nInternal mechanism of DeepSOR T\\nUnlike SOR T, deepSOR T incorporates appearance-based re-identification to increase the\\ntracking robustness. It includes:\\n• F eature Extraction: Utilizes a CNN that encodes a visual embedding of each object.'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 24, 'page_label': '15'}, page_content='tracking robustness. It includes:\\n• F eature Extraction: Utilizes a CNN that encodes a visual embedding of each object.\\n• State Estimation: Implements Kalaman filter similar to SOR T.\\n• Data Association: combines visual apperance (consine distance) and motion (Ma-\\nhalanobis distance) in the cost matrix to match detection between frames\\n15'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 25, 'page_label': '16'}, page_content='Anuj Bhandari: Capstone Project I\\nJustification for BinHero\\n• SOR T is a lightweight tracking alogrithm which requires minimal computational\\nresources and is able to perform eﬀiciently in real-time.This allows it to provide\\nan immediate user feedback in real time when users perform an action of disposal\\nwithout overloading or slowing down.\\n• DeepSOR T is more computationally heavy algorithm which is deployed on a server.\\nIt can cope with complex tasks with even greater accuracy and robustness (like\\nmultiple users or occlusion) using appearance-based Re-ID.\\nT able 3.1: Algorithm and Model Selection Summary and Justification\\nRequirement Solution Reason for Selection\\nReal-time Object Detec-\\ntion\\nYOLO11, YOLO11n Rapid single-pass detection with\\nhigh accuracy\\nMobile Inference YOLO11n supoorts offline usage via tflite, op-\\nerate on mid-range mobile phones.\\nMotion verification for Dis-\\nposing actions\\nSOR T, DeepSOR T V erifies object movement into the\\nbin with a consistent temporal pat-\\ntern.'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 25, 'page_label': '16'}, page_content='erate on mid-range mobile phones.\\nMotion verification for Dis-\\nposing actions\\nSOR T, DeepSOR T V erifies object movement into the\\nbin with a consistent temporal pat-\\ntern.\\nOcclusion robustness DeepSOR T manages scenarios with several\\nusers and increases the precision of\\nbehavioral analysis\\n3.2.2 Mathematical F ormulation of YOLO11\\nYOLO11 ( Y ou Only Look Once, version 11) is an object detection model, which can\\npredict bounding and class probability of an object using full image and having only\\none evaluation. Although the mathematical foundations remain a modification of the\\nprevious YOLO architecture design, it maintains the main detection and loss concept\\nthat made its precursors so particular.\\n3.2.3 Model Output\\nGiven an input image, YOLO11 divides it into an S ×S grid. F or each grid cell, the\\nmodel predicts B bounding boxes, each with:\\n• Center coordinates (x, y)\\n• Width w and height h\\n• Objectness confidence score C\\n16'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 26, 'page_label': '17'}, page_content='Anuj Bhandari: Capstone Project I\\n• Class probabilities for C classes: p1, p2, . . . , p C\\nThe output tensor per grid cell is:\\nOutput = [x1, y1, w1, h1, C1, . . . , x B, yB, wB, hB, CB, p1, . . . , p C]\\n3.2.4 Loss F unction\\nThe overall loss L in YOLO11 is a sum of localization, objectness, and classification\\nlosses:\\nL = λcoord\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n⊮obj\\nij\\n[\\n(xi −ˆxi)2 + (yi −ˆyi)2 + (wi −ˆwi)2 + (hi −ˆhi)2\\n]\\n+\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n⊮obj\\nij (Ci −ˆCi)2\\n+ λnoobj\\nS2\\n∑\\ni=0\\nB∑\\nj=0\\n⊮noobj\\nij (Ci −ˆCi)2\\n+\\nS2\\n∑\\ni=0\\n⊮obj\\ni\\n∑\\nc∈classes\\n(pi(c) −ˆpi(c))2\\nWhere:\\n• 1obj\\nij = 1 if an object appears in cell i and bounding box j is responsible for the\\nprediction, otherwise 0.\\n• ⊮noobj\\nij = 1 if no object appears in cell i for bounding box j\\n• λcoord and λnoobj are hyperparameters to balance the loss terms\\n• (xi, yi, wi, hi, Ci, pi(c)) are the predicted values\\n• (ˆxi, ˆyi, ˆwi, ˆhi, ˆCi, ˆpi(c)) are the ground truth values\\n3.2.5 Performance Metrics\\nThe standard object detection metrics are used to assess YOLO11.'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 26, 'page_label': '17'}, page_content='• (ˆxi, ˆyi, ˆwi, ˆhi, ˆCi, ˆpi(c)) are the ground truth values\\n3.2.5 Performance Metrics\\nThe standard object detection metrics are used to assess YOLO11.\\n• Precision, Recall, F1 Score\\n• A verage Precision (AP)\\n• Mean A verage Precision (mAP): commonly reported as mAP@0.5 and mAP@0.5:0.95\\n17'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 27, 'page_label': '18'}, page_content='Anuj Bhandari: Capstone Project I\\n3.2.6 Performance Considerations and T rade-offs\\nThe models selected illustrate a harmony between operational limitations and perfor-\\nmance. T able 3.2 consists of a comparative discussion of the key components of AI that\\nare incorporated into the BinHero platform\\nT able 3.2: Comparison of AI components in BinHero\\nF actor YOLO11\\n(Server)\\nYOLO11n\\n(Mobile)\\nDeepSOR T\\n(Server)\\nSOR T (Mo-\\nbile)\\nAccuracy High Moderate High Low\\nInference Speed F ast, GPU usage 30 FPS Real-Time V ery F ast\\nModel Size Large <10MB Medium V ery small\\nResource Demand High Low High V ery low\\n3.3 Requirement specification\\n3.3.1 F unctional Requirement\\n1. W aste Classification Accuracy\\nT o provide feedback to the systems with a high correlation, and to allow implemen-\\ntation of an effective system of scoring users, an object detection model would need\\nto perform classification with an accuracy of at least 85% on the biodegradable,\\nnon-biodegradable, and general waste bins.'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 27, 'page_label': '18'}, page_content='to perform classification with an accuracy of at least 85% on the biodegradable,\\nnon-biodegradable, and general waste bins.\\n2. Real-Time Object Detection\\nThe server-side version of YOLO11 is expected to show inference rates of at least 30\\nframes per second, whereas the mobile version of YOLO11n is required to reach at\\nleast 20 frames per second so that the on-device training could be responsive. This\\nperformance profile ensures that the integration with the camera-initiated disposal\\nevents takes place without any problems.\\n3. Motion Based T rash Disposal V alidation\\nThe system should ensure that a waste item sensed in the hands of the user proceeds\\nto the bin. This is done through the use of motion tracking (using SOR T on a mobile\\ndevice and DeepSOR T on a server) to explicitly connect the identity of the objects\\nacross frames by checking their location trajectories.\\n4. Preprocessing pipeline\\nA set of preprocessing manipulations, i.e., resizing, normalization, and motion-blur\\n18'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 28, 'page_label': '19'}, page_content='Anuj Bhandari: Capstone Project I\\nfiltering, is required on each captured frame before inference. This is necessary to\\nmaintain the quality of data and ensure consistency in inference when deploying\\nmobile devices and servers.\\n5. Hybrid Inference Architecture\\nThe system must enable the execution of two modes of AI:\\n• Mobile: On device YOLO11n and SOR T.\\n• Server: YOLO11 + DeepSOR T inference in the cloud.\\nThe backup mechanism to ensure continuity of inference in the event of unreacha-\\nbility at the server should be adopted.\\n6. Action based Reward T riggering\\nUpon confirmation by the tracking algorithm that an object has been properly\\ndisposed of, multiple actions should follow: the user should have their point tally\\nautomatically updated, positive feedback should be triggered, and the event of\\nsuccessful disposal should be recorded in order to be analyzed later.\\n3.3.2 Non-F unctional Requirements\\n1. Inference Speed'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 28, 'page_label': '19'}, page_content='successful disposal should be recorded in order to be analyzed later.\\n3.3.2 Non-F unctional Requirements\\n1. Inference Speed\\nOn the server-side, inference cannot have a latency greater than 200 milliseconds,\\nand mobile-side inference must be less than 100 milliseconds per frame. These\\nlimitations ensure that feedback can be rendered in real time in disposal activities.\\n2. Model Precision and Recall Thresholds\\nThe precision has to be higher than 0.85, and the recall must be 0.80 to implement\\nthe use of an object detection model in practice. These metrics are crucial to\\nprevent false positives and guarantee confidence in AI-generated feedback.\\n3. Scalability of AI Pipeline\\nThe architecture has to support the horizontal scalability of the cloud-based infer-\\nence API such that multiple requests from different users can be enabled to run\\nconcurrently . The server is recommended to enable support for batch inference and\\nGPU parallelism.\\n4. Data Privacy and Security'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 28, 'page_label': '19'}, page_content='concurrently . The server is recommended to enable support for batch inference and\\nGPU parallelism.\\n4. Data Privacy and Security\\nAll images captured by the user will be processed fully in memory , except where the\\nuser specifies the permanent storage permission. Their interface should use both an\\nauthentication scheme and thorough input sanitization against adversarial attacks.\\n19'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 29, 'page_label': '20'}, page_content='Chapter 4\\nSystem Design\\n4.1 System Architecture Diagram\\nThe architecture of this application is designed as a modular, AI-powered mobile appli-\\ncation that integrates with user interface, real-time location services, real-time commu-\\nnications, cloud-based services, and smart inference pipelines. It is developed to support\\na hybrid AI inference model with the ability to perform computation both on mobile de-\\nvices and dedicated servers to maximize the performance, accuracy , and responsiveness.\\nThis section provides a comprehensive analysis of all the parts, the AI pipeline, model\\nworkflows, and the ability of systems to interact.\\n4.1.1 High-Level Overview\\nAt the top level, the entire system is a cross-platform mobile app which we initially\\nconceptualized in Figma and then built using React Native. The service is connected\\nto several modules: the Google Maps API to geolocate bins, the W ebSockets library to'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 29, 'page_label': '20'}, page_content='conceptualized in Figma and then built using React Native. The service is connected\\nto several modules: the Google Maps API to geolocate bins, the W ebSockets library to\\ninteract in real-time, a dual-mode AI inference stack (mobile and server), a Flask-based\\nAI service, an Express.js back-end, and a MongoDB database. The AI modules become\\nthe real-time motion-tracking and object-detection components of the system, thus the\\nsmart core of the system.\\n4.1.2 F rontend and User Interaction Layer\\nThe mobile application serves as the main user interface, allowing users to:\\n• record waste disposal in the form of a video.\\n• View AI validation in real-time.\\n• Keep track of points and take part in gamified activities.\\n• Geotag bin locations\\nBuilt using React Native, ensuring cross-platform compatibility with Android and iOS.\\nFigma is used to design the prototypes of the design and the user journey with a focus\\non its accessibility and the readability of interaction.\\n20'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 30, 'page_label': '21'}, page_content='Anuj Bhandari: Capstone Project I\\n4.1.3 Geolocation and Real-Time Communication\\nGoogle Maps API allows bin mapping, route tracking, and spatial data visualization. At\\nthe same time, W ebSockets provide low-latency communication between the application\\nand the backend services. The architecture allows processing motion updates along with\\nAI detection triggers and the related inference feedback in real time.\\nUI/UX\\nFigma\\nFRONTEND\\nR eact Nativ e\\nApp\\nGEOL OC A TION\\nGoogle\\nMaps API\\nREAL TIME\\nW ebSock et s\\nMobile Inf er ence\\nY OL Ov11n\\n+\\nSORT\\nOpenCV\\nD A T ABA SE\\nMongoDB\\nAI Inf er ence Ser v er\\nY OL Ov11 + DeepSORT\\nBA CKEND\\nExpr ess.js\\nSer vice\\nFlask\\nOpenCV\\nFigure 4.1: System Architecture\\n4.1.4 AI Inference Architecture\\nThe AI Module of the project utilizes PyT orch to perform as the intelligence engine, and\\nis designed in two modes, including mobile-side and server-side. OpenCV is completely\\nintegrated within both operating modes to simplify acquisition of frames, preprocessing,'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 30, 'page_label': '21'}, page_content='is designed in two modes, including mobile-side and server-side. OpenCV is completely\\nintegrated within both operating modes to simplify acquisition of frames, preprocessing,\\nand visualization on a real-time basis.\\n21'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 31, 'page_label': '22'}, page_content='Anuj Bhandari: Capstone Project I\\nMobile-side inference\\nA quantized and lightweight version of YOLO11. YOLO11n is applied at the final stage,\\nlocally , to make inferences. It is optimized for edge inference, having the following char-\\nacteristics:\\n• Smaller convolutional layers with diminished depth multipliers.\\n• T ensorflow compatibility for mobile acceleration.\\n• Real-time detection at approximately 30 FPS.\\nBewley et al. (2016) introduced SOR T (Simple Online and Realtime T racking). It uses a\\nKalman filter and Hungarian matching algorithm to relate bounding boxes across frames,\\nthus making local motion analysis (a key requirement in detection of disposal actions,\\neven where connectivity is low. It was utilized for motion tracking on the client-side.\\nLikewise, Libraries like OpenCV is utilized to capture frames from the device’s cameras,\\nand apply various preprocessing techniques such as resizing, normalizing, etc.\\nServer-side inference'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 31, 'page_label': '22'}, page_content='Likewise, Libraries like OpenCV is utilized to capture frames from the device’s cameras,\\nand apply various preprocessing techniques such as resizing, normalizing, etc.\\nServer-side inference\\nComputationally more demanding work is redirected towards a cloud-based AI Inference\\nServer that accommodates:\\n• YOLO11 for a better precision on object detection.\\n• DeepSOR T for a robust multi-target tracking, it uses both spatial and visual fea-\\ntures, suggested by Re-ID, and embeddings.\\n• A Flask microservice to handle stages such as preprocessing, batch inference, and\\npost-processing of some end-to-end workflow.\\nThe frames acquired by the mobile application get sent to the server with the help of\\nW ebSockets or REST endpoints. T o maintain the interaction, the inference server maps\\nthe AI pipeline and provides classification scores, motion vectors, and confidence scores.\\nThese results are then channelled out to the front-end to justify action and feedback.\\n4.1.5 AI Pipeline W orkflow'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 31, 'page_label': '22'}, page_content='These results are then channelled out to the front-end to justify action and feedback.\\n4.1.5 AI Pipeline W orkflow\\nThe system can be considered modular, and it includes the following AI blocks:\\n• Image Acquisition: During the disposal action mobile device camera is used to\\nrecord frames. It would be triggered through the application’s UI. The OpenCV is\\nutilised to capture each frame of a video stream in real-time and then process it.\\n• Pre-processing: All the captured frames undergo various preprocessing steps such\\nas normalization and resizing. F rames are queued for server-based inference to\\nmaximize the GPU batch processing. The step also filters out frames with motion\\n22'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 32, 'page_label': '23'}, page_content='Anuj Bhandari: Capstone Project I\\nblur or bad lighting to preserve the input quality . OpenCV fulfills these tasks on\\nboth the mobile and server sides and thus makes the frame quality and model\\ncompatibility consistent.\\n• Object Detection: The YOLO models classify the items into three classes: biodegrad-\\nable and non-biodegradable. Bounding boxes, confidence scores, and probabilities\\nare examples of detection outputs. The module runs inference at about 30 frames\\nper second, guaranteeing real-time performance for server and mobile versions.\\n• T racking: The detections received by the system are sent to SOR T/DeepSOR T,\\nand the movement vectors are obtained to check whether the object passed or not\\nthe user’s hands to the bin. Using a tracker, each object in subsequent frames is\\ngiven a unique identity , thus providing consistency of time in the motion analysis\\nprocess.\\n• Decision Logic : Disposal validation logic is a compliance rule that checks an'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 32, 'page_label': '23'}, page_content='given a unique identity , thus providing consistency of time in the motion analysis\\nprocess.\\n• Decision Logic : Disposal validation logic is a compliance rule that checks an\\nidentified waste to determine whether it meets a reasonable disposal trajectory .\\nThe reasoning uses spatial thresholds and velocity approximations to prevent false\\nconfirmations and rewards points to the score, activates responsive feedback, or\\ncauses corrective directions.\\n4.1.6 Backend and Database\\nExpress.js back-end acts as a middleware, which performs the following main functions:\\n• Authentication and API routing\\n• Flow of data between frontend, AI server, and database.\\n• User session management.\\nInformation about the users, including their profile, is stored in the MongoDB database\\nalong with metatags of bin locations, the classification history , and gamification data. It\\nis flexible in processing the outputs of AI and heterogeneous data types in virtue of its\\nNoSQL architecture.'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 32, 'page_label': '23'}, page_content='is flexible in processing the outputs of AI and heterogeneous data types in virtue of its\\nNoSQL architecture.\\nThe combination of real-time artificial intelligence with an optimally designed backend\\nframework and its responsive user interface will make the proposed architecture deliver\\nscalable, accurate, and community-driven waste disposal solutions. The AI pipeline of hy-\\nbrid inference allows the system to dynamically match the flexibilities to user situational\\ncontexts, preserving its performance and reliability aspect in smart waste management.\\n23'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 33, 'page_label': '24'}, page_content='Anuj Bhandari: Capstone Project I\\n4.2 Use Case Diagram, UML Diagram, and Other Visual Rep-\\nresentations\\nUnified Modeling Language (UML) diagrams are a mandatory tool in ensuring a seamless\\nintegration of artificial-intelligence-based modules into the system structure. UML can\\nfacilitate this because it provides a graphical framework through which the relationship\\nbetween algorithmic development and the requirements of the end-user can be articulated,\\nand the capabilities of functions like motion tracking and object identification can be\\nbrought into line with functional need and the practicality of actual implementations.\\nUML diagrams also help determine trigger points of artificial intelligence inference, data\\nflow, and feedback mechanisms, and hence serve as a pillar in the system planning and\\nimplementation.\\n4.2.1 Use Case Diagram\\nThe figure 4.2 provides extensive information about the relations between the actors of'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 33, 'page_label': '24'}, page_content='implementation.\\n4.2.1 Use Case Diagram\\nThe figure 4.2 provides extensive information about the relations between the actors of\\nthe application and the embedded AI parts. The following are some of the primary actors:\\n• User: The system connects to artificial-intelligence elements, such as the ones that\\nscan the waste objects, receive the results of object detection, and give feedback\\non the accuracy of the process in real-time. The interactions trigger AI pipelines,\\nwhich are exemplified by YOLO11 in object classification and DeepSOR T or SOR T\\nin motion prove.\\n• Admin: The Administrator has the management of the backend, which should\\ninvolve the approval of bin placements, administration of users, and moderation of\\ncontent. Even though these administration roles do not have any direct interaction\\nwith the AI processes, they still build on the given model by investigating the\\nresults of the detection and measuring the effectiveness of user input.'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 33, 'page_label': '24'}, page_content='with the AI processes, they still build on the given model by investigating the\\nresults of the detection and measuring the effectiveness of user input.\\n• Contributor: The task of contributing bin locations, verifying them, which provides\\nthe imprint of proper behavior during removal/disposal within specific areas, that\\nwill increase the accuracy of detection within geographical locations.\\nA Use Case Diagram helps identify important exchanges where the system initiates or\\nconsumes AI services. The use case of Scan W aste, in its turn, calls the object detection\\nmodel, and conversely , Receive Real-Time F eedback is closely connected with the veri-\\nfication logic of motion tracking. Moreover, the transformations of specific gamification\\nuse-cases, e.g., the “Earn Points and Rewards” and the “T rack Progress,” have begun to\\nbe powered by AI feedback loops to evaluate the discarding precision and frequency to'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 33, 'page_label': '24'}, page_content='use-cases, e.g., the “Earn Points and Rewards” and the “T rack Progress,” have begun to\\nbe powered by AI feedback loops to evaluate the discarding precision and frequency to\\ntranslate your behavioral information into effective motivators to be recognized by the\\nusers.\\n24'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 34, 'page_label': '25'}, page_content='Anuj Bhandari: Capstone Project I\\nConfigure Rewards and \\nChallenges\\nManage Users\\nModerate Content\\nView System Reports\\nApprove Bin Submissions\\nAdmin\\nAdd Bin Location\\nContributor\\nVerify Bin Location\\nUser\\nLogin\\nRegister\\nScan Waste\\nJoin Team Challenges\\nJoin PvP Challenges\\nView Leaderboard\\nTrack Progress\\nReceive Real-Time\\nFeedback\\nView Detection Result\\nEarn Points & Rewards\\n<<includes>>\\n<<includes>>\\n<<includes>>\\n<<includes>>\\nBinHero: An AI-Driven Smart Waste Disposal Mobile App with Object Detection & Motion Tracking  \\nFigure 4.2: Use Case Diagram\\n25'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 35, 'page_label': '26'}, page_content='Anuj Bhandari: Capstone Project I\\n4.3 Class Diagram\\nThe Figure 4.3 represents the system’s class diagram, which follows a modular, layered\\ndesign that clearly distinguishes the fields of user-interaction, artificial-intelligence pro-\\ncessing, game mechanics, and data management. At the core of the user structure is\\nan abstract User class, which serves as the base, and functionalities that are added to\\nthe contributor and Admin, respectively , to have functionalities on role-specific duties.\\nThe main classes defining the architecture of the application include the Bin, Challenge,\\nand GamificationEngine, although the latter is by far the most important, as it is the\\nagent system used to encourage user action. In cooperation with these entities, there are\\nAI-centered classes, in particular AIModel and W asteDetection, that recognize and get\\nimage inputs and award points to the user with the help of waste recognition detection.'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 35, 'page_label': '26'}, page_content='AI-centered classes, in particular AIModel and W asteDetection, that recognize and get\\nimage inputs and award points to the user with the help of waste recognition detection.\\nSupporting components such as DatabaseManager and MapService cultivate the funda-\\nmental functionalities: data persistence and location-based services. Through inheritance\\nand associations, the object-oriented methodologies get threaded into the design, hence\\nensuring the reuse of code and the sustainability of systems.\\nCore Logic\\n1..*\\n1..*\\n1..*\\n1..*\\n1\\n1..*\\nstores\\n1..*\\nUser Roles\\n1..*\\nUser\\n- userID: String\\n- username: String\\n- email: String\\n- password: String\\n- role: String\\n- points: int\\n+ register()\\n+ login()   \\n+ updateProfile()\\n+ participateInChallenge()\\n+ getLeaderboard\\nContributor\\n+ submitBinLocation()\\n+ verifyBinLocation()\\nAdmin\\n+ manageUsers()\\n+ moderateContent()\\n+ approveBinSubmission()\\n+ configureChallenges()\\nBin\\n- binID: String\\n- location: String\\n- type: String\\n- verifiedBy: String\\n+ addBin()'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 35, 'page_label': '26'}, page_content='+ verifyBinLocation()\\nAdmin\\n+ manageUsers()\\n+ moderateContent()\\n+ approveBinSubmission()\\n+ configureChallenges()\\nBin\\n- binID: String\\n- location: String\\n- type: String\\n- verifiedBy: String\\n+ addBin()\\n+ submitBin()\\n+ verifyBin()\\n+ reportBinStatus()\\nChallenge\\n- challengeID: String\\n- description: String\\n- type: String\\n- participants: List<User>\\n- status: String\\n- startTime: DateTime\\n+ joinChallenge(userID)\\n+ completeChallenge(userID)\\n+ getResults()\\n+ rewardWinner()\\nGamificationEngine\\n- userPoints: Map<UserID, int>\\n- leaderboard: List <User>\\n- challengeList: List<Challenge>\\n+ awardPoints(userID, points)\\n+ updateLeaderboard()\\n+ createChallenge()\\n+ trackProgress(userID)\\nWasteDetection\\n- detectionID: String\\n- image: Image\\n- timeStamp: DateTime\\n- pointsAwarded: int\\n+ sendImage()\\n+ receiveResult()\\n+ updateScore()\\nDatabseManager\\n- connectionString: String\\n+ saveUser(user: User)\\n+ fetchBins()\\n+ saveChallenge(challenge: Challenge)\\n+ updatePoints( userID, newPoints)\\nSystem Services\\n1\\nAIModel'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 35, 'page_label': '26'}, page_content='+ updateScore()\\nDatabseManager\\n- connectionString: String\\n+ saveUser(user: User)\\n+ fetchBins()\\n+ saveChallenge(challenge: Challenge)\\n+ updatePoints( userID, newPoints)\\nSystem Services\\n1\\nAIModel\\n- modelType: String\\n- accuracy: Float\\n- version: String\\n+ detectWaste()\\n+ trackMotion()\\n+ validateDisposal()\\n+ returnResult()\\nMapService\\n- binLocations: List<Bin>\\n- userLocation: String\\n+ showNearbyBins()\\n+ updateMap()\\n+ calculateDistance()\\n1 1..*\\n1\\n1\\n11\\nupdates score\\ntriggers\\njoins\\nsubmits\\nverifies\\n1 1..*\\n1\\nconfigures1\\npowers\\nstores\\nstores logs of\\nsyncs bins for\\nlogs\\nstores\\nis displayed on\\n1..*\\nsaves data for\\nFigure 4.3: Class Diagram\\n26'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 36, 'page_label': '27'}, page_content='Anuj Bhandari: Capstone Project I\\n4.4 Activity Diagram\\nThe given activity diagrams outline the main processes within the project system. Figure\\n4.4 illustrates the user authentication process, detailing secure registration and logging\\nprocedures. Likewise, Figure 4.5 depicts PvP challenge flow, where the user interaction\\nis guided by AI functionality , such as object detection and real-time score.\\n1. User Authentication Flow\\nUser Authentication Flow forms the key mechanism with which individuals will log\\ninto the application. On starting the application, the user is shown the choice of\\neither creating a new account or logging in to an existing one. A user must fill in\\na registration form and deliver all the required personal data at registration, but\\nonly valid credentials are necessary in the process of following the subsequent login\\nprocess. The system matches the received data with existing data and, based on'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 36, 'page_label': '27'}, page_content='only valid credentials are necessary in the process of following the subsequent login\\nprocess. The system matches the received data with existing data and, based on\\nthe results, enables access or sends the error message to correct. After the authen-\\ntication procedure has been executed, a user goes through the process of setting\\nup permissions and arranging his or her profile, then goes to the main interface.\\nThe flow ensures an orderly and secure onboarding of all users, thus preserving\\nthe privacy and integrity of the application. While this flow does not incorporate\\nAI functionality , it still serves as a fundamental mechanism of controlling access\\nto the more advanced functionalities of the system. The authentication process is\\nalso imperative in the formation of user-specific sessions, thereby making it easy to\\naccurately track participation, scoring, and behavior in the system. Therefore, the\\nuser Authentication Flow is crucial for customized and responsible user experiences'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 36, 'page_label': '27'}, page_content='accurately track participation, scoring, and behavior in the system. Therefore, the\\nuser Authentication Flow is crucial for customized and responsible user experiences\\nin the larger ecosystem.\\n2. PvP Challenge Flow\\nPvP Challenge Flow reflects the gamified procedure of waste disposal that en-\\ncourages users to follow environmentally responsible practices through interactive\\ngameplay , making it an essential component of the application. Users can navi-\\ngate to various challenge sections once they are authenticated. The system first\\nexamines the eligibility of an applicant according to set standards; once this is\\napproved, the system assigns the applicant a suitable opponent and also indicates\\nthat the challenge has begun. At the same time, AI-enabled waste detection units\\nwill monitor and track the user’s waste disposal actions in real time, and then they\\nwill score the user in real time based on accuracy and overall performance. The'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 36, 'page_label': '27'}, page_content='will monitor and track the user’s waste disposal actions in real time, and then they\\nwill score the user in real time based on accuracy and overall performance. The\\nfigure 4.5 depicts how the integration of AI into a structured interaction cycle is\\nsmooth. The system still observes the session, determines whether the exchange\\nhas ended based on either time limits or scoring points then it declares a winner\\nusing automated reasoning. After the challenge is over, rewards are calculated, and\\nthe scoreboard position is updated. The PvP challenge flow shows how the project\\nimplements AI to observe user behavior data in real time, offer continuous feedback\\n27'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 37, 'page_label': '28'}, page_content='Anuj Bhandari: Capstone Project I\\nStart\\nUser launches app\\nChoose Register or Login\\nRegister Login\\nEnter Form Data Enter Email & Password\\nCredentials \\nValid? Show Error Login\\nPermission & Profile Setup Screens\\nHome Screen\\nNo\\nYes\\nFigure 4.4: User Authentication Activity Diagram\\n28'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 38, 'page_label': '29'}, page_content='Anuj Bhandari: Capstone Project I\\nto users, and keep them engaged with gamification. It reinvents the very process\\nof waste management by transforming all the following steps of disposing of waste\\nmatter into a rather interactive process moderated by intelligent automation and\\nthe sense of behavioral responsibility .\\n3. W aste Disposal V alidation Flow\\nThe figure 4.6 illustrates an organized sequence of interactions that incorporates AI\\nfeatures into every phase of user involvement. The sequence starts once a user steps\\nin front of a trash bin, which triggers a system to send out its camera and capture\\nan image of a waste object, and send the obtained visual data to the backend server.\\nAn artificial-intelligence model on a server or in a mobile device performs the task\\nof waste classification by determining the object to be in one of two categories:\\nbiodegradable or non-biodegradable. This classification gives the framework for'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 38, 'page_label': '29'}, page_content='of waste classification by determining the object to be in one of two categories:\\nbiodegradable or non-biodegradable. This classification gives the framework for\\nmaking informed decisions, making sure that each action includes an understand-\\ning of its context. Simultaneously , they use a motion-tracking module that tracks\\nthe user as he or she performs the disposal action to ensure that the detected ob-\\nject is dropped into the bin. Once this verification is done, the system evaluates\\nthe result. Upon valid disposal, the system will grant the corresponding points,\\nincrement the user’s score and reward, and the disposal will be recorded to be anal-\\nysed later. In case the disposal does not pass verification, no reward is received\\nby the user, as a consequence, promoting accountability and adequate behavior.\\nThe results of the user input are shown within the system as soon as the process\\nis performed, with the prevalent gamification features of the achievement badges'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 38, 'page_label': '29'}, page_content='The results of the user input are shown within the system as soon as the process\\nis performed, with the prevalent gamification features of the achievement badges\\nor progress notifications. According to the diagram, artificial-intelligence abilities,\\nincluding classification, tracking, and decision-making, have been well assimilated\\ninto the system workflow. The given integration enables automatic reaction to a\\ngreat range of stimuli, provision of immediate feedback, and the assuring level of\\nuser involvement. All of its components not only improve the performance of the\\noperations but also lead to a responsive, smart, and user-oriented system architec-\\nture.\\n4.5 Sequence Diagram\\nThe sequence diagram highlights the use of AI technology to address real-time decision-\\nmaking and user experience by documenting the past interactions of users, backend ser-\\nvices, and AI-enabled devices. This process starts by either logging in or registering'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 38, 'page_label': '29'}, page_content='making and user experience by documenting the past interactions of users, backend ser-\\nvices, and AI-enabled devices. This process starts by either logging in or registering\\nthrough the mobile application. This application directly contacts the Auth API and the\\nassociated database to confirm the credentials entered. After a user is validated, he/she\\nis placed in a Player-versus-Player game, where the participant creates communications\\nwith the Matchmaker Service, the W ebSocket Hub, and an Opponent. In the course of\\ngameplay , the user spots a waste item and scans it. The picture is sent to the backend,\\n29'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 39, 'page_label': '30'}, page_content='Anuj Bhandari: Capstone Project I\\nUser logs in and opens challenges\\nJoin PvP Challenge\\nEligible?\\nAssign opponent & start match\\nWaste detection & live score\\nTimer/Score\\nended?\\nCalculate Winner & Rewards\\nUpdate leaderboard\\nReturn to Dashboard/MenuShow eligibility message\\nNo Yes\\nNo\\nYes\\nFigure 4.5: PvP Challenges Activity Diagram\\n30'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 40, 'page_label': '31'}, page_content='Anuj Bhandari: Capstone Project I\\nOpens Camera\\nCapture Waste Image\\nSend to Backend\\nAI Classify\\nMotion Tracking\\nDisposal Valid?\\nNo\\nAward Points and Log\\nUpdate score and rewards\\nShow result\\nYes\\nFigure 4.6: W aste Disposal V alidation Activity Diagram\\n31'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 41, 'page_label': '32'}, page_content='Anuj Bhandari: Capstone Project I\\nUser\\nMobile App Auth API Auth\\nService Database Challenge\\nAPI\\nMatchmaker\\nService WebSocket\\nHub Opponent AI ServiceGamificationBin API Admin\\nLogin or \\nRegister1\\ndisplay \\ndashboard or \\nshow error\\n9 Join PvP\\nPOST / Login / \\nRegister\\n3 validateUser()\\n4\\nSELECT user \\nWHERE email = ?\\n5user + hashedPw\\n6\\nresult \\n(success | fail)\\n7\\n200 OK + JWT \\n| 401 Error\\nPOST / join {userID}\\n11 findOpponent()\\n12 Create PvP Room\\n13Room Connected\\n14 Room Connected\\nWaste Scanned\\n16 Validate Image\\n17Score\\n18Update Score\\n19 Update Score\\n15\\n8\\n10\\n2\\nloop\\n20Match Result\\n21Winner / Rewards\\n22\\nShow Match \\nResult\\n23 Scan Item\\n24 Send Image\\n25Classification\\n26 Award Points\\n27New Total\\n28\\nReward \\nAnimation\\n29 Open Add Bin\\n30 Add Bin\\n31Pending\\n32\\nReview / \\nApprove\\n33Notify / Push Bin Verified\\n34\\nShow Toast Bin \\nis Live \\nUser\\nMobile App Auth API Auth\\nService Database Challenge\\nAPI\\nMatchmaker\\nService WebSocket\\nHub Opponent AI ServiceGamificationBin API Admin\\nFigure 4.7: Sequence Diagram'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 41, 'page_label': '32'}, page_content='34\\nShow Toast Bin \\nis Live \\nUser\\nMobile App Auth API Auth\\nService Database Challenge\\nAPI\\nMatchmaker\\nService WebSocket\\nHub Opponent AI ServiceGamificationBin API Admin\\nFigure 4.7: Sequence Diagram\\nwhere it is categorized using AI. The system checks the input and computes a score and\\nposts a user profile through the Gamification service. The application presents perfor-\\nmance analytics, awards, and outcomes by using a fun animated presentation. The users\\ncan also use the registration flow of bin, a nd the submissions are checked and verified\\nby the Admin and the Bin API before making the content live. This diagram shows the\\nsmooth combination of all AI functions of classification and scoring into the larger and\\nreceptive architecture of the service to support the real-time and gamified waste disposal\\nexperience.\\n4.6 Pseudocode\\nThis section gives a breakdown of the key algorithms that lie beneath the main com-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 41, 'page_label': '32'}, page_content='experience.\\n4.6 Pseudocode\\nThis section gives a breakdown of the key algorithms that lie beneath the main com-\\nponents of our application, namely object detection based on the YOLO11 model and\\nreal-time tracking that relies on SOR T and DeepSOR T. The pseudocode details the main\\nsequential steps by which the YOLO11 model can be trained to distinguish different\\ntypes of waste, pre- and exported to an edge device with support for real-time tracking\\nto maintain the object identity across frames. Collectively , they comprise the main part\\nof the AI in the work of the system, as it allows detecting waste items and garbage bins\\n32'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 42, 'page_label': '33'}, page_content='Anuj Bhandari: Capstone Project I\\nwith high accuracy in dynamic settings and monitoring them over time.\\nThe pseudocode 1 discusses fine-tuning the YOLO11 object-detection model on one of\\nthe custom datasets that contain such classes as biodegradable waste, non-biodegradable\\nwaste, and garbage bins. It involves downloading a pre-trained model, preparing the\\ndataset to achieve the objectives of the detection task, and then training the model for a\\nfew epochs to fine-tune it to its purpose.\\nThe pseudocode 2 provided below leads to exporting the pre-trained model (YOLO11n)\\nto the T ensorFlow Lite version to enable the use of the model on an edge device and de-\\npicts the reloading procedure using on-device inference. This process is mobile-friendly ,\\nand the model still has the capability of detecting.\\nThe next pseudocode 3 represents the pseudocode of the implementation of the SOR T\\n(Simple Online and Realtime T racking) method of simultaneous and multiple object'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 42, 'page_label': '33'}, page_content='The next pseudocode 3 represents the pseudocode of the implementation of the SOR T\\n(Simple Online and Realtime T racking) method of simultaneous and multiple object\\ntracking. It shows how the detections that are produced by a real-time object detector,\\nincluding the YOLO11n, are passed to the SOR T tracker. The algorithm processes all\\nframes of the video stream and constantly updates and displays the tracking data of every\\nobject.\\nAlgorithm 1 YOLO11 Fine-T uning for W aste Classification\\nThis pseudocode 4 describes how DeepSOR T, a more complex algorithm that improves\\nthe SOR T algorithm by adding deep appearance feature embedding, works. It outlines\\nthe inference of detections of a model like YOLO11, where the feature extractor gener-\\nates embeddings that are then used by the tracker in aligning and maintaining track of\\nthe identity of the objects across adjacent frames. This kind of use increases tracking\\naccuracy , particularly in highly populated or partially obstructed scenes.'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 42, 'page_label': '33'}, page_content='the identity of the objects across adjacent frames. This kind of use increases tracking\\naccuracy , particularly in highly populated or partially obstructed scenes.\\n1: Input: Pre-trained model, dataset config\\n2: Output: T rained weights, metrics\\n▷ Import YOLO module\\n3: Import YOLO from ultralytics\\n▷ Load model\\n4: MODEL ←YOLO(\"yolo11.pt\")\\n▷ Set dataset config\\n5: DATA_CONFIG ←\"dataset/data.yaml\"\\n▷ Set training params\\n6: NUM_EPOCHS ←20\\n7: IMAGE_SIZE ←640\\n8: DEVICE_ID ←0\\n▷ Start training\\n9: TRAINING_RESULTS ←MODEL.train(\\n10: data=DATA_CONFIG,\\n11: epochs=NUM_EPOCHS,\\n12: imgsz=IMAGE_SIZE,\\n13: device=DEVICE_ID\\n14: )\\n15: Print(\"Training completed. Check output folder.\")\\n33'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 43, 'page_label': '34'}, page_content='Anuj Bhandari: Capstone Project I\\nAlgorithm 2 YOLO11n Export to T ensorFlow Lite\\n1: Input: Custom pre-trained YOLO11n model\\n2: Output: TFLite model ready for inference\\n▷ Import YOLO\\n3: Import YOLO from ultralytics\\n▷ Load model\\n4: MODEL ←YOLO(\"yolo11n.pt\")\\n▷ Set export format\\n5: EXPORT_FORMAT ←\"tflite\"\\n▷ Export model\\n6: MODEL.export(format=EXPORT_FORMAT)\\n▷ Load TFLite model\\n7: TFLITE_MODEL ←YOLO(\"yolo11n_float32.tflite\")\\n▷ Ready for inference\\nAlgorithm 3 Object T racking with YOLO11 and SOR T\\n1: Input: Video stream, pre-trained detector\\n2: Output: T racked object visualizations\\n▷ Import modules\\n3: Import video_stream\\n4: Import object_detector\\n5: Import SORTTracker\\n▷ Initialize detector\\n6: DETECTOR ←LoadYOLO11(weights=\"YOLO11n.pt\", classes=3)\\n▷ Initialize tracker\\n7: TRACKER ←SORT()\\n▷ Open video stream\\n8: VIDEO ←OpenVideoStream(\"input_video.mp4\")\\n9: while VIDEO.has_frames() do\\n10: FRAME ←VIDEO.read_next_frame()\\n▷ Run detection\\n11: DETECTIONS ←DETECTOR.detect(FRAME)\\n▷ Update tracker'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 43, 'page_label': '34'}, page_content='8: VIDEO ←OpenVideoStream(\"input_video.mp4\")\\n9: while VIDEO.has_frames() do\\n10: FRAME ←VIDEO.read_next_frame()\\n▷ Run detection\\n11: DETECTIONS ←DETECTOR.detect(FRAME)\\n▷ Update tracker\\n12: TRACKED_OBJECTS ←TRACKER.update(DETECTIONS)\\n▷ Draw tracking results\\n13: for all object in TRACKED_OBJECTS do\\n14: Draw bounding_box on FRAME\\n15: Draw track_id on FRAME\\n16: end for\\n17: DISPLAY FRAME\\n18: end while\\n▷ Release resources\\n19: VIDEO.release()\\n20: Close display_window\\n34'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 44, 'page_label': '35'}, page_content='Anuj Bhandari: Capstone Project I\\nAlgorithm 4 Object T racking with YOLO11 and DeepSOR T\\n1: Input: Video stream, detector, appearance model\\n2: Output: T racked object visualizations\\n▷ Import modules\\n3: Import video_stream\\n4: Import object_detector\\n5: Import DeepSORTTracker\\n6: Import AppearanceFeatureExtractor\\n▷ Initialize detector\\n7: DETECTOR ←LoadYOLO11(\"YOLO11n.pt\", classes=3)\\n▷ Initialize DeepSOR T\\n8: APPEARANCE_ENCODER ←LoadFeatureExtractor(\"cnn_encoder.onnx\")\\n9: TRACKER ←DeepSORT(appearance_model=APPEARANCE_ENCODER)\\n▷ Open video\\n10: VIDEO ←OpenVideoStream(\"input_video.mp4\")\\n11: while VIDEO.has_frames() do\\n12: FRAME ←VIDEO.read_next_frame()\\n▷ Run detection\\n13: DETECTIONS ←DETECTOR.detect(FRAME)\\n▷ Extract features\\n14: FEATURES ←[ ]\\n15: for all bbox in DETECTIONS do\\n16: CROP ←CropImage(FRAME, bbox)\\n17: FEATURE ←APPEARANCE_ENCODER.extract(CROP)\\n18: Append FEATURE to FEATURES\\n19: end for\\n▷ Combine data\\n20: DETECTIONS_WITH_FEATURES ←Combine(DETECTIONS, FEATURES)\\n▷ Update tracker'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 44, 'page_label': '35'}, page_content='17: FEATURE ←APPEARANCE_ENCODER.extract(CROP)\\n18: Append FEATURE to FEATURES\\n19: end for\\n▷ Combine data\\n20: DETECTIONS_WITH_FEATURES ←Combine(DETECTIONS, FEATURES)\\n▷ Update tracker\\n21: TRACKED_OBJECTS ←TRACKER.update(DETECTIONS_WITH_FEATURES)\\n▷ Draw results\\n22: for all object in TRACKED_OBJECTS do\\n23: Draw bounding_box on FRAME\\n24: Draw track_id on FRAME\\n25: end for\\n26: DISPLAY FRAME\\n27: end while\\n▷ Release resources\\n28: VIDEO.release()\\n29: Close display_window\\n35'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 45, 'page_label': '36'}, page_content='Chapter 5\\nConclusion\\nThe BinHero project is an example of how AI systems can be used with practical effec-\\ntiveness on pressing environmental issues, as it provides real-time waste classification and\\nconfirms behavioral patterns. The combination of motion tracking and object detection\\nallows the system to authenticate the activities of users and promote the appropriate\\ndisposal practice. The integration of game elements, such as PvP and reward systems,\\ntakes user activity to new levels and supports achieving sustainable environment behav-\\nior as well. The architecture is scalable, mobile, and data privacy-friendly , and offers\\na community that finds a responsive solution to urban waste management. F uture ad-\\nvancements will incorporate features like government cooperation, linguistic support, and\\nAR integration, which may increase its usefulness.\\n36'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 46, 'page_label': '37'}, page_content='References\\nMohammed M Abo-Zahhad and Mohammed Abo-Zahhad. Real time intelligent garbage\\nmonitoring and eﬀicient collection using yolov8 and yolov5 deep learning models for\\nenvironmental sustainability .Scientific Reports, 15(1):1–26, 2025.\\nEaman Alharbi, Ghadah Alsulami, Sarah Aljohani, W aad Alharbi, and Somayah Al-\\nbaradei. Real-time detection and monitoring of public littering behavior using deep\\nlearning for a sustainable environment. Scientific Reports, 15(1):3000, 2025.\\nAli Arishi. Real-time household waste detection and classification for sustainable recy-\\ncling: A deep learning approach. Sustainability, 17(5):1902, 2025.\\nAlex Bewley , ZongY uan Ge, Lionel Ott, F abio Ramos, and Ben Upcroft. Simple online\\nand realtime tracking. CoRR, abs/1602.00763, 2016. URL http://arxiv.org/abs/1602.\\n00763.\\nW enhao Huang, Kazuhiro Mikami, Yin Chen, and Jin Nakazawa. Real-time image-based\\nautomotive sensing: A practice on fine-grained garbage disposal. In Proceedings of the'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 46, 'page_label': '37'}, page_content='00763.\\nW enhao Huang, Kazuhiro Mikami, Yin Chen, and Jin Nakazawa. Real-time image-based\\nautomotive sensing: A practice on fine-grained garbage disposal. In Proceedings of the\\n13th International Conference on the Internet of Things , pages 138–145, 2023.\\nSilpa Kaza, Lisa Y ao, Perinaz Bhada-T ata, and F rank V an W oerden.What a waste 2.0:\\na global snapshot of solid waste management to 2050 . W orld Bank Publications, 2018.\\nRahima Khanam and Muhammad Hussain. Y olov11: An overview of the key architectural\\nenhancements. arXiv preprint arXiv:2410.17725 , 2024.\\nY eji Kim and Jeongho Cho. Aidm-strat: augmented illegal dumping monitoring strategy\\nthrough deep neural network-based spatial separation attention of garbage. Sensors,\\n22(22):8819, 2022.\\nEverest Z Kuang, Kushal Raj Bhandari, and Jianxi Gao. Optimizing waste man-\\nagement with advanced object detection for garbage classification. arXiv preprint\\narXiv:2410.09975, 2024.'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 46, 'page_label': '37'}, page_content='Everest Z Kuang, Kushal Raj Bhandari, and Jianxi Gao. Optimizing waste man-\\nagement with advanced object detection for garbage classification. arXiv preprint\\narXiv:2410.09975, 2024.\\nMd Shahariar Nafiz, Shuvra Smaran Das, Md Kishor Morol, Abdullah Al Juabir, and Dip\\nNandi. Convowaste: An automatic waste segregation machine using deep learning. In\\n2023 3rd International conference on robotics, electrical and signal processing techniques\\n(ICREST), pages 181–186. IEEE, 2023.\\n37'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 47, 'page_label': '38'}, page_content='Anuj Bhandari: Capstone Project I\\nImane Nedjar, Mohammed M’hamedi, and Mokhtaria Bekkaoui. Real-time solid waste\\nsorting machine based on deep learning.International journal of electrical and computer\\nengineering systems, 15(7):581–589, 2024.\\nNupur Pathak, Gangotri Biswal, Megha Goushal, V raj Mistry , Palak Shah, F englian Li,\\nand Jerry Gao. Smart city community watch—camera-based community watch for\\ntraﬀic and illegal dumping. Smart Cities, 7(4):2232–2257, 2024.\\nC Arun Prasath. Automated waste classification and segregation using yolov5 and robotic\\narm. F rontiers in Mathematical and Computational Research, pages 8–15, 2025.\\nF aizul Rakib Sayem, Md Sakib Bin Islam, Mansura Naznine, Mohammad Nashbat,\\nMazhar Hasan-Zia, Ali K Ansaruddin Kunju, Amith Khandakar, Azad Ashraf,\\nMolla Ehsanul Majid, Saad Bin Abul Kashem, et al. Enhancing waste sorting and\\nrecycling eﬀiciency: robust deep learning-based approach for classification and detec-'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 47, 'page_label': '38'}, page_content='Molla Ehsanul Majid, Saad Bin Abul Kashem, et al. Enhancing waste sorting and\\nrecycling eﬀiciency: robust deep learning-based approach for classification and detec-\\ntion. Neural Computing and Applications , 37(6):4567–4583, 2025.\\nGary White, Christian Cabrera, Andrei Palade, F an Li, and Siobhan Clarke. W astenet:\\nW aste classification at the edge for smart bins.arXiv preprint arXiv:2006.05873, 2020.\\n38'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 48, 'page_label': '39'}, page_content='Appendices\\n39'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 49, 'page_label': '40'}, page_content='Appendix A\\nGantt Chart and Current Progress\\nA.1 Gantt Chart\\nMarch\\nApril\\nMay\\nJune\\nJuly\\nAugust\\nSeptember\\nOctober\\nNovember\\nDecember\\nProject \\nKick-off \\nand \\nTopic \\nSelection\\nRequirement \\nGathering \\nand \\nAnalysis\\nTechnical \\nResearch \\nand \\nProject \\nProposal\\nUI/UX \\nDesign\\nFrontend \\nDevelopment\\nBackend \\nDevelopment \\nand \\nDatabase \\nDesign\\nAI \\nModel \\nresearch \\nand \\nImplementation\\nGamification \\nSystem \\nDevelopment\\nCrowdsourced \\nBin \\nMapping \\nand \\nLocation \\nVerification\\nApp \\nTesting\\nFinal \\nreview, \\nDocumentation \\nand \\nDeployment\\n10th \\nMarch \\n- \\n18th \\nMarch\\n19th \\nMarch \\n- \\n21th \\nMarch\\n22th \\nMarch \\n- \\n30th \\nMarch\\n1st \\nApril \\n- \\n1st \\nMay\\n2nd \\nMay  \\n- \\n25th \\nJune\\n26th \\nJune \\n- \\n21st \\nAugust\\n22nd \\nAugust \\n- \\n25th \\nSeptember\\n26th \\nSeptember \\n- \\n20th \\nOctober\\n21st \\nOctober \\n- \\n4th \\nNovember\\n5th \\nNovember \\n- \\n16th \\nNovember\\n17th \\nNovember \\n- \\n6th \\nDecember\\nRequirement \\nGathering \\nand \\nAnalysis\\nTechnical \\nResearch \\n& \\nProject \\nProposal\\nUI/UX \\nDesign\\nFrontend \\nDevelopment\\nBackend \\nDevelopment \\n& \\nDatabase'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 49, 'page_label': '40'}, page_content='16th \\nNovember\\n17th \\nNovember \\n- \\n6th \\nDecember\\nRequirement \\nGathering \\nand \\nAnalysis\\nTechnical \\nResearch \\n& \\nProject \\nProposal\\nUI/UX \\nDesign\\nFrontend \\nDevelopment\\nBackend \\nDevelopment \\n& \\nDatabase \\nDesign\\nAI \\nModel \\nresearch \\nand \\nImplementation\\nGamification \\nSystem \\nDevelopment\\nBin \\nMapping \\n&    \\nLocation \\nVerification\\nApp \\nTesting\\nFinal \\nreview, \\nDocumentation \\n& \\nDeployment\\nYangma \\nLama\\nAnuj \\nBhandari\\nAayush \\nKarki\\nPrabin \\nJoshi\\nAayush \\nBasnet\\nMembers \\nResponsible:\\nProject \\nKick-off \\nand \\nTopic \\nSelection\\nMarch\\nApril\\nMay\\nJune\\nJuly\\nAugust\\nSeptember\\nOctober\\nNovember\\nDecember\\nFigure A.1: Project Milestone Gantt Chart\\n40'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 50, 'page_label': '41'}, page_content='Anuj Bhandari: Capstone Project I\\nFigure A.2: YOLO11 T raining\\nA.2 Current Progress and Preliminary Results\\nT able A.1: Current Performance of Detection Model\\nClass Images Instances Precision Recall mAP@0.5 mAP@0.5:0.95\\nAll 2098 17624 0.789 0.659 0.745 0.506\\nBiodegradable 676 13637 0.763 0.591 0.685 0.405\\nNon-Biodegradable 1049 3987 0.814 0.726 0.805 0.608\\nInference speed: 13.3ms per image.\\n41'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 51, 'page_label': '42'}, page_content='Appendix B\\nLog Sheet\\n42'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 52, 'page_label': '43'}, page_content='Anuj Bhandari: Capstone Project I\\nFigure B.1: Meeting 1\\n43'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 53, 'page_label': '44'}, page_content='Anuj Bhandari: Capstone Project I\\nFigure B.2: Meeting 2\\n44'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 54, 'page_label': '45'}, page_content='Anuj Bhandari: Capstone Project I\\nFigure B.3: Meeting 3\\n45'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 55, 'page_label': '46'}, page_content='Anuj Bhandari: Capstone Project I\\nFigure B.4: Meeting 4\\n46'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 56, 'page_label': '47'}, page_content='Anuj Bhandari: Capstone Project I\\nFigure B.5: Meeting 5\\n47'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 57, 'page_label': '48'}, page_content='Anuj Bhandari: Capstone Project I\\nFigure B.6: Meeting 6\\n48'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 58, 'page_label': '49'}, page_content='Anuj Bhandari: Capstone Project I\\nFigure B.7: Meeting 7\\n49'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 59, 'page_label': '50'}, page_content='Anuj Bhandari: Capstone Project I\\nFigure B.8: Meeting 8\\n50'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 60, 'page_label': '51'}, page_content='Anuj Bhandari: Capstone Project I\\nFigure B.9: Meeting 9\\n51'),\n",
       " Document(metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 61, 'page_label': '52'}, page_content='Anuj Bhandari: Capstone Project I\\nFigure B.10: Meeting 10\\n52')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "268d2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c61d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "db = FAISS.from_documents(docs,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc8533eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anuj Bhandari: Capstone Project I\n",
      "Justification for BinHero\n",
      "• SOR T is a lightweight tracking alogrithm which requires minimal computational\n",
      "resources and is able to perform eﬀiciently in real-time.This allows it to provide\n",
      "an immediate user feedback in real time when users perform an action of disposal\n",
      "without overloading or slowing down.\n",
      "• DeepSOR T is more computationally heavy algorithm which is deployed on a server.\n",
      "It can cope with complex tasks with even greater accuracy and robustness (like\n",
      "multiple users or occlusion) using appearance-based Re-ID.\n",
      "T able 3.1: Algorithm and Model Selection Summary and Justification\n",
      "Requirement Solution Reason for Selection\n",
      "Real-time Object Detec-\n",
      "tion\n",
      "YOLO11, YOLO11n Rapid single-pass detection with\n",
      "high accuracy\n",
      "Mobile Inference YOLO11n supoorts offline usage via tflite, op-\n",
      "erate on mid-range mobile phones.\n",
      "Motion verification for Dis-\n",
      "posing actions\n",
      "SOR T, DeepSOR T V erifies object movement into the\n",
      "bin with a consistent temporal pat-\n",
      "tern.\n"
     ]
    }
   ],
   "source": [
    "# Similarity search\n",
    "# results = db.similarity_search(\"JavaScript in JSX with curly braces \", k=1)\n",
    "query = \"Which YOLO is used in BinHero ?\"\n",
    "\n",
    "results = db.similarity_search(query, k=1)\n",
    "\n",
    "print(results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7e9d69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", temperature=0.9) # 'temperature' controls the randomness of the model's output; higher values make responses more creative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a4e62b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n Question: {input}\\n\\n'), additional_kwargs={})])\n",
       "| ChatGoogleGenerativeAI(model='models/gemini-2.5-pro', google_api_key=SecretStr('**********'), temperature=0.9, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000002BB65DD4550>, default_metadata=())\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retrieval Chain, Document chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    " Question: {input}\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243c6290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, you can use curly braces in your JSX to “open a window” to JavaScript. This allows you to add JavaScript logic or reference a dynamic property inside the HTML-like markup.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from langchain_core.documents import Document\n",
    "# res = document_chain.invoke({\n",
    "#     \"input\":\"How do curly braces work in JSX?\",\n",
    "#     \"context\": [Document(page_content=\"\"\"JavaScript in JSX with curly braces \n",
    "# JSX lets you write HTML-like markup inside a JavaScript file, keeping rendering logic and content in the same place. Sometimes you will want to add a little JavaScript logic or reference a dynamic property inside that markup. In this situation, you can use curly braces in your JSX to “open a window” to JavaScript:\n",
    "# const person = {\n",
    "#   name: 'Gregorio Y. Zara',\n",
    "#   theme: {\n",
    "#     backgroundColor: 'black',\n",
    "#     color: 'pink'\n",
    "#   }\n",
    "# };\n",
    "\n",
    "# export default function TodoList() {\n",
    "#   return (\n",
    "#     <div style={person.theme}>\n",
    "#       <h1>{person.name}'s Todos</h1>\n",
    "#       <img\n",
    "#         className=\"avatar\"\n",
    "#         src=\"https://i.imgur.com/7vQD0fPs.jpg\"\n",
    "#         alt=\"Gregorio Y. Zara\"\n",
    "#       />\n",
    "#       <ul>\n",
    "#         <li>Improve the videophone</li>\n",
    "#         <li>Prepare aeronautics lectures</li>\n",
    "#         <li>Work on the alcohol-fuelled engine</li>\n",
    "#       </ul>\n",
    "#     </div>\n",
    "#   );\n",
    "# }\n",
    "\n",
    "\n",
    "# \"\"\")]\n",
    "# })\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98a9cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd359c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retriever_chain = create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e07f9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002BB65610EC0>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n Question: {input}\\n\\n'), additional_kwargs={})])\n",
       "            | ChatGoogleGenerativeAI(model='models/gemini-2.5-pro', google_api_key=SecretStr('**********'), temperature=0.9, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000002BB65DD4550>, default_metadata=())\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0f2d8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Which YOLO is used in BinHero ?', 'context': [Document(id='f1ea425f-1e7d-4cb5-8c5a-58fec561c509', metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 25, 'page_label': '16'}, page_content='Anuj Bhandari: Capstone Project I\\nJustification for BinHero\\n• SOR T is a lightweight tracking alogrithm which requires minimal computational\\nresources and is able to perform eﬀiciently in real-time.This allows it to provide\\nan immediate user feedback in real time when users perform an action of disposal\\nwithout overloading or slowing down.\\n• DeepSOR T is more computationally heavy algorithm which is deployed on a server.\\nIt can cope with complex tasks with even greater accuracy and robustness (like\\nmultiple users or occlusion) using appearance-based Re-ID.\\nT able 3.1: Algorithm and Model Selection Summary and Justification\\nRequirement Solution Reason for Selection\\nReal-time Object Detec-\\ntion\\nYOLO11, YOLO11n Rapid single-pass detection with\\nhigh accuracy\\nMobile Inference YOLO11n supoorts offline usage via tflite, op-\\nerate on mid-range mobile phones.\\nMotion verification for Dis-\\nposing actions\\nSOR T, DeepSOR T V erifies object movement into the\\nbin with a consistent temporal pat-\\ntern.'), Document(id='bcd57f14-fabb-4e65-af1c-b5fa4856eb61', metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 20, 'page_label': '11'}, page_content='Anuj Bhandari: Capstone Project I\\nT able 2.1: Summary of Related W orks\\nStudy Problem Addressed AI Approach Deployment Approach Key Result Relevance to BinHero\\nArishi(2025) accurate classification of\\nhousehold garbage\\nYOLOv8-CBAM Consumer devices 89.5% mAP Ensures the application of YOLO\\non-device real-time classification\\nKuang et al.(2024) YOLO performance in practi-\\ncal settings\\nYOLOv8 Controlled vs real environ-\\nments\\nHigh accuracy in clean settingsEndorses the real-time applicability\\nof YOLO\\nWhite et al.(2020) On-device waste classificationW asteNet (CNN) Jetson Nano 97% accuracy Evidence of low- resource edge de-\\nployment\\nNafiz et al.(2023) Multi-category trash classifi-\\ncation\\nInception-ResNet V2T ransfer learning on limited\\ndata\\n98% accuracy Confirms pre-trained CNNs on\\nsmall datasets\\nNedjar et al.(2024) Light models for sorting on\\nembedded systems\\nMobileNet, NASNet-\\nMobile\\nRaspberry Pi with TFLite Nearly 100% accuracyV alidates lightweight models for em-\\nbedded inference'), Document(id='903daa8c-96c8-4543-bac9-afade12acb2d', metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 27, 'page_label': '18'}, page_content='Anuj Bhandari: Capstone Project I\\n3.2.6 Performance Considerations and T rade-offs\\nThe models selected illustrate a harmony between operational limitations and perfor-\\nmance. T able 3.2 consists of a comparative discussion of the key components of AI that\\nare incorporated into the BinHero platform\\nT able 3.2: Comparison of AI components in BinHero\\nF actor YOLO11\\n(Server)\\nYOLO11n\\n(Mobile)\\nDeepSOR T\\n(Server)\\nSOR T (Mo-\\nbile)\\nAccuracy High Moderate High Low\\nInference Speed F ast, GPU usage 30 FPS Real-Time V ery F ast\\nModel Size Large <10MB Medium V ery small\\nResource Demand High Low High V ery low\\n3.3 Requirement specification\\n3.3.1 F unctional Requirement\\n1. W aste Classification Accuracy\\nT o provide feedback to the systems with a high correlation, and to allow implemen-\\ntation of an effective system of scoring users, an object detection model would need\\nto perform classification with an accuracy of at least 85% on the biodegradable,\\nnon-biodegradable, and general waste bins.'), Document(id='887863b1-642e-4414-9465-41a7506ae932', metadata={'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-07-24T15:08:47+00:00', 'source': 'GROUP2_ANUJ_BHANDARI_0348445_CP1.pdf', 'total_pages': 62, 'page': 24, 'page_label': '15'}, page_content='Anuj Bhandari: Capstone Project I\\nJustification for BinHero\\nThe selection of these algorithms and models are based on:\\n• The project is dependent on real-time classification as a core business in order to\\nprovide real-time feedback to its users.\\n• YOLO11 provides high quality , reliable performance in closed, server-based condi-\\ntions where GPU adequate resources are available.\\n• YOLO11n makes mobile classification possible even for mid-range devices. While\\nalso providing offline support and a lower server burden.\\n• The application’s requirement to handle dynamic trash disposal actions in a mat-\\nter of seconds matches with its anchor-based infrastructure and the fast inference\\ncapabilities.\\nMotion T racking\\nWhile waste classification is an important factor for validating disposal behavior, it is\\nstill insuﬀicient on its own. The application empolys motion tracking with two tracking\\nalgorithms, DeepSOR T and SOR T, that links object IDs across frames, to make sure that')], 'answer': 'Based on the provided context, BinHero uses **YOLO11** and **YOLO11n**.\\n\\nSpecifically:\\n*   **YOLO11** is deployed on a server.\\n*   **YOLO11n** is used for mobile inference and supports offline usage.'}\n",
      "Based on the provided context, BinHero uses **YOLO11** and **YOLO11n**.\n",
      "\n",
      "Specifically:\n",
      "*   **YOLO11** is deployed on a server.\n",
      "*   **YOLO11n** is used for mobile inference and supports offline usage.\n"
     ]
    }
   ],
   "source": [
    "response = retriever_chain.invoke({\"input\": query})\n",
    "print(response)  # See the full response structure\n",
    "\n",
    "# If the answer is under the \"answer\" key:\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
